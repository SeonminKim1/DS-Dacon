{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dacon - 퇴근시간 버스 승차 인원 예측 1등 코드 리뷰\n",
    "### (Jeju Island Bus Passengers Estimation in Leaving work time)\n",
    "(https://dacon.io/competitions/official/229255/codeshare/511?page=1&dtype=recent)\n",
    "- 같이 첨부된 pdf 파일에 대한 저작권은 전적으로 위 링크 1등팀에 있습니다.\n",
    "\n",
    "### 목적 \n",
    "- **<U> 18시부터 20시까지의 역별 버스 승차인원</U> 구하기 **   \n",
    "\n",
    "### 데이터 설명\n",
    "#### train, test  공통 사항\n",
    "- 해당 데이터에는 버스카드를 통해 결제를 한 경우에 대한 정류소 승, 하차 데이터로 모든 승차정보의 경우는 기록이 되어있지만,\n",
    "- 버스에서 하차를 할 때, 버스카드를 찍지 않는 경우, 해당 기록이 비어 있는 상태입니다. 따라서, 승차 인원수와 하차 인원수가 동일하지 않고 다소 차이가 있음을 미리 알려드립니다.\n",
    "\n",
    "#### train, test csv 공통사항\n",
    "- 해당 버스정류장에 대한 각각의 위도, 경도가 제공이 되어있는 상태로 같은 정류장 이름이지만 위도와 경도가 서로 다른 경우가 존재합니다. \n",
    "- 해당 경우는, 같은 정류장 이름을 가지고 있는 길 건너편의 정류장에 해당이 됩니다.\n",
    "\n",
    "#### train.csv and test.csv\n",
    "- train.csv 의 경우, <U>**2019년 9월 제주도의 각 날짜, 출근시간 (6시~12시)의 버스 정류장별 승하차 인원**</U>,  <U>**퇴근시간(18시~20시)의 버스 정류장별 승차 인원**</U>이 기록되어 있습니다.\n",
    "- test.csv의 경우, <U>**2019년 10월의 각 날짜, 출근시간(오전 6시~12시)의 버스 정류장별 승하차 인원**</U>이 기록되어  있음.\n",
    "\n",
    "#### submission_제출양식.csv \n",
    "- submission_제출양식의 경우, test data의 ID와 목표변수인 18시~20시 승차 인원로만 이루어져 있습니다.\n",
    "- 참가자분들께서는 test.csv에서 ID와 예측값을 결합하여, 해당 submission_제출양식.csv 파일형식 처럼 만든 이후,\n",
    "- 해당 제출 파일 양식 그대로(변수명 포함된 상태) 제출을 해주시면 됩니다\n",
    "\n",
    "### 변수 설명\n",
    "- id(고유번호=인덱스), date(날짜), bus_route_id(버스경로 id)\n",
    "- in_out(시내버스, 시외버스), station_code(정거장 번호 id)\n",
    "- station_name(정거장 이름), latitude(위도), longitude(경도)\n",
    "- 6-7_ride, 7-8_ride, 9-10_ride, 10-11_ride, 11-12_ride (오전시간 탑승인원)\n",
    "- 6-7_takeoff, 7-8_takeoff, 9-10_takeoff, 10-11_takeoff, 11-12_takeoff (오전시간 하차 인원)\n",
    "- 18-20_ride (퇴근시간 탑승인원)\n",
    "\n",
    "### EDA Idea\n",
    "- 변수 섞어서 쓴 이유\n",
    "- BUS_ROUTE_ID / VHC_ID  -> GROUP BY 분석 용이하게 위해서, / 혹은 교호작용? 좀더 작게도보고 크게도 보고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# data preprocessing\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 130, 'max_rows', 30)\n",
    "\n",
    "# ignore warining\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# save\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.chdir(\"C:\\\\Users\\\\urse\\\\Desktop\\\\dataset\\\\Dacon-BusJeju\")\n",
    "         \n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "bts = pd.read_csv(\"bus_bts.csv\")\n",
    "\n",
    "print('데이터 로드 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bts.shape)\n",
    "bts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling Code\n",
    "- chromedriver.exe를 작업 경로에 위치시켜야 함\n",
    "- 2019년 9월 1일부터 2019년 10월 16일 까지의 데이터를 크롤링하였으며, 오전 10시의 관측 데이터만 사용하였기 때문에 data leakage에 해당하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium\n",
    "# Chrome Web driver 설치 필요 lib 파일에 넣어놨음\n",
    "from selenium.webdriver import Chrome # \n",
    "\n",
    "def crawl_weather() :\n",
    "    weather_data_10 = pd.DataFrame(columns=['현재일기_10','현재기온_10'])\n",
    "                                            #,'체감온도_10','일강수_10'])\n",
    "    browser = Chrome('C:/Users/urse/Anaconda3/envs/untitled/Lib/site-packages/chromedriver.exe')\n",
    "    url = 'http://www.weather.go.kr/weather/observation/currentweather.jsp?auto_man=m&type=t99&reg=184&tm=2019.10.25.16%3A00&x=19&y=7'\n",
    "    browser.get(url)\n",
    "\n",
    "    # 19년 9월 1일부터 ~ 19년 10월 16일까지 \n",
    "    for i in range(0,46):\n",
    "        #print(i)\n",
    "        i+=1\n",
    "\n",
    "        elem = browser.find_element_by_id('observation_text')\n",
    "        elem.clear()\n",
    "        elem.send_keys(\"2019.9.{}.10:00\".format(i))\n",
    "        \n",
    "        btn = browser.find_elements_by_class_name('btn')\n",
    "        btn[2].click()\n",
    "        \n",
    "        time.sleep(1) # 1을 줘야 제대로 크롤링이 됨.\n",
    "        weathers = browser.find_elements_by_css_selector('td')\n",
    "        weather_data_10 = weather_data_10.append(\n",
    "            pd.DataFrame([[weathers[40].text, weathers[44].text]],\n",
    "                           #,weathers[46].text, weathers[47].text]], # 체감온도_10, 일강수_10\n",
    "                         columns=['현재일기_10','현재기온_10']))\n",
    "                                  #,'체감온도_10','일강수_10']))\n",
    "        \n",
    "    print('success !')\n",
    "    browser.close()\n",
    "    \n",
    "    return weather_data_10\n",
    "\n",
    "# 크롤링 진행 및 csv 파일 만들기\n",
    "if os.path.isfile('weather.csv') == False:\n",
    "    weather_data = crawl_weather()\n",
    "    weather_data.to_csv('weather.csv', index = False, encoding='euc-kr')\n",
    "    print('save.. !')\n",
    "else:\n",
    "    weather_data = pd.read_csv('weather.csv', engine='python', encoding='euc-kr')\n",
    "\n",
    "# 원본코드는 현재일기_10, 현재기온_10, 체감온도_10, 일강수_10 중 체감온도_10과 일강수_10이 데이터가 너무 높고, \n",
    "weather_data\n",
    "\n",
    "# EDA의 요일에 따른 탑승객 수 요일별로 시각화 해보는 과정 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 탐색적 자료분석 (Exploratory Data Analysis)\n",
    "### (1-1) 요일에 따른 18시 ~ 20시 탑승객 수\n",
    "#### 분석결과\n",
    "- 요일에 따라 탑승객 수가 확연히 차이나고, 특히 주중과 주밀이 큰 차이가 남.\n",
    "- 월요일에서 금요일로 갈수록 탑승객 수가 줄다가, 다시 주말에 탑승객 수가 점점 올라감.\n",
    "\n",
    "#### 피쳐 생성\n",
    "- weekday - date 변수를 datetime으로 type 변경 후, 요일을 dummy 변수 생성\n",
    "- weekend - weekday 변수에서 주중0, 주말1\n",
    "- 1820_w_mean - 요일별 18 ~ 20시의 탑승인원의 평균\n",
    "- 1820_w_sum - 요일별 18 ~ 20시의 탑승인원의 합\n",
    "\n",
    "![EDA_1-1](./img/EDA_1-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA 1-1 -> datetime 으로 변환 및 요일 변환\n",
    "# train data\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['weekday'] = train['date'].dt.weekday\n",
    "\n",
    "# test data\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['weekday'] = test['date'].dt.weekday\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train['weekday'] # 0 ~ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - # 버스경로 id + 정거장번호 id 만들기\n",
    "# route_station str로 변환 ( 전부다 정수형이였음 )\n",
    "# Train_data\n",
    "train['bus_route_id'] = train['bus_route_id'].astype(str)\n",
    "train['station_code'] = train['station_code'].astype(str)\n",
    "\n",
    "# train data - bus_route_id + station_code\n",
    "train['route_station'] = train['bus_route_id'] + ',' + train['station_code']\n",
    "\n",
    "# Test_data\n",
    "test['bus_route_id'] = test['bus_route_id'].astype(str)\n",
    "test['station_code'] = test['station_code'].astype(str)\n",
    "\n",
    "# test data - bus_route_id + station_code\n",
    "test['route_station'] = test['bus_route_id'] + ',' + test['station_code'] \n",
    "\n",
    "# route_station => 버스경로와 정거장 id 를 합침으로서 버스경로별 정거장과의 상관관계를 인정하는 듯한?\n",
    "print(train.shape, test.shape)\n",
    "train['route_station'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bus_route_id_weekday => 버스경로 id + 요일(0~6) / bus_route_id + weekday \n",
    "train['bus_route_id_weekday'] = train['bus_route_id'].astype(str) + ',' + train['weekday'].astype(str) \n",
    "test['bus_route_id_weekday'] = test['bus_route_id'].astype(str) + ',' + test['weekday'].astype(str) \n",
    "\n",
    "# 파생변수\n",
    "train['bus_route_id_weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_code_weekday => 역 정거장 코드 + 요일(0~6)\n",
    "# 해당 정거장에서의 요일별 지수\n",
    "train['station_code_weekday'] = train['station_code'].astype(str) + ',' + train['weekday'].astype(str)\n",
    "test['station_code_weekday'] = test['station_code'].astype(str) + ',' + test['weekday'].astype(str)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train['station_code_weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_station_weekday => 버스경로, 역id  +  요일(0~6) 여부\n",
    "train['route_station_weekday'] = train['route_station'].astype(str) + ',' + train['weekday'].astype(str) \n",
    "test['route_station_weekday'] = test['route_station'].astype(str) + ',' + test['weekday'].astype(str)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train['route_station_weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bts.shape)\n",
    "bts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_time\n",
    "# bts 데이터는 승객 데이터\n",
    "# bts 데이터를 통해서 실제 시간대 구함.\n",
    "# bts.csv 데이터에서 geton_time 열에서 시간대만 추출하여 on_time 컬럼을 만듬\n",
    "# geton_time => 승객드이 탄 시간\n",
    "# 시분초 데이터를 시간으로만\n",
    "# 06:34:45 -> 06\n",
    "bts['on_time']  = bts['geton_time'].apply(lambda x : x[:2])\n",
    "\n",
    "# on_time 가변수 생성\n",
    "bts.iloc[bts.query('on_time == \"06\"').index,13] = '6~7_ride'\n",
    "bts.iloc[bts.query('on_time == \"07\"').index,13] = '7~8_ride'\n",
    "bts.iloc[bts.query('on_time == \"08\"').index,13] = '8~9_ride'\n",
    "bts.iloc[bts.query('on_time == \"09\"').index,13] = '9~10_ride'\n",
    "bts.iloc[bts.query('on_time == \"10\"').index,13] = '10~11_ride'\n",
    "bts.iloc[bts.query('on_time == \"11\"').index,13] = '11~12_ride'\n",
    "\n",
    "print(bts.shape)\n",
    "bts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-2) 출근시간 승차인원과 퇴근시간 승차인원\n",
    "#### 분석결과\n",
    "- 출근시간과 퇴근시간의 승차인원 합과 평균 / \n",
    "- 6-8시 출근시간의 승차인원 합과 평균이 18-20시 퇴근시간 승차인원의 합과 평균과 비슷한 양상을 띰, 즉 버스를 타고 출근한 인원은 퇴근 후에도 버스를 타고 귀가함.\n",
    "\n",
    "#### 피쳐 생성\n",
    "- (출근시간 승차인원)68a, 810a, 1012a, t ~ t+2 기준의 정보를 담고 있음.\n",
    "- (출근시간 승차인원)69a, 912a, t ~ t+3 기준의 정보를 담고 있음.\n",
    "![EDA_1-2](./img/EDA_1-2.jpg)\n",
    "\n",
    "### (1-3) 출근시간 하차인원과 퇴근시간 승차인원\n",
    "#### 분석결과\n",
    "- 출근시간의 하차인원 하차인원 합과 평균 \n",
    "- 퇴근시간 하차인원의 합과 평균이 비슷한 양상을 띰, 즉 출근시간 하차인원이 많은 곳에서는 퇴근시간 승차인원이 많을 것으로 예상됨.\n",
    "\n",
    "#### 피쳐생성\n",
    "- (출근시간 하차인원)68b, 810b, 1012b, t~t+2 기준의 정보를 담고 있음.\n",
    "- (출근시간 하차인원)69b, 912b, t~ t+3 기준의 정보를 담고 있음.\n",
    "![EDA_1-3](./img/EDA_1-3.jpg)\n",
    "\n",
    "### (1-4) 오전시간대 승/하차인원과 퇴근시간 승차인원\n",
    "#### 분석결과\n",
    "- 오전시간에 승/하차 인원이 많다는 것은 그날의 유동인구가 많다는 것을 뜻함.\n",
    "- 주말이나 연휴에는 유동인구 자체가 적고, 주중에는 상대적으로 유동인구가 많음.\n",
    "\n",
    "#### 피쳐 생성\n",
    "- ride_sum : 오전시간 승차인원 합\n",
    "- takeoff_sum - 오전시간 하차인원 합\n",
    "![EDA_1-4](./img/EDA_1-4.jpg)\n",
    "\n",
    "### (1-5) 배차 간격에 따른 퇴근시간 탐승객 수\n",
    "#### 분석결과\n",
    "- 배차 간격에 따른 퇴근시간 탑승객 수 사이의 패턴을 명확히 보기 위해 로그변환을 함\n",
    "- 수요가 많은 버스일수록 배차간격이 짧고, \n",
    "- 그에 따라 배차 간격이 짧을 수록 퇴근시간 탑승객 수가 많음.\n",
    "\n",
    "#### 피쳐 생성\n",
    "- bus_interval - bus_route_id 별 배차간격의 평균 값\n",
    "- Scale에 무관한 Tree 기반 모델을 사용했기 때문에 실제 변수는 log 변환을 하지 않음.\n",
    "![EDA_1-5](./img/EDA_1-5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 승 하차 시간대 통합 변수 (t ~ t+2)\n",
    "# t~t+1, t+1~t+2 시간대 승하차인원을 합하여 t~t+2 시간대 승하차인원 변수를 만듬\n",
    "\n",
    "# train data - 승차시간 합\n",
    "train['68a']=train['6~7_ride']+train['7~8_ride'] \n",
    "train['810a']=train['8~9_ride']+train['9~10_ride']\n",
    "train['1012a']=train['10~11_ride']+train['11~12_ride']\n",
    "\n",
    "# train data - 하차시간 합\n",
    "train['68b']=train['6~7_takeoff']+train['7~8_takeoff'] \n",
    "train['810b']=train['8~9_takeoff']+train['9~10_takeoff']\n",
    "train['1012b']=train['10~11_takeoff']+train['11~12_takeoff']\n",
    "\n",
    "# test data - 승차시간 합\n",
    "test['68a']=test['6~7_ride']+test['7~8_ride']\n",
    "test['810a']=test['8~9_ride']+test['9~10_ride']\n",
    "test['1012a']=test['10~11_ride']+test['11~12_ride']\n",
    "\n",
    "# test data - 하차시간 합\n",
    "test['68b']=test['6~7_takeoff']+test['7~8_takeoff']\n",
    "test['810b']=test['8~9_takeoff']+test['9~10_takeoff']\n",
    "test['1012b']=test['10~11_takeoff']+test['11~12_takeoff']\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make features by using target variable\n",
    "- 우리가 최종적으로 예측해야할 것은 각 일자별(date), 버스 노선(bus_route_id) 상의 정류장(station_name)의 퇴근시간 하차인원(18~20_ride)이다.\n",
    "\n",
    "- **bus_route_id, station_name, weekday의 각 조합별 퇴근시간 하차인원(18~20_ride)의 여러 통계량을 구한 후 이를 train set, test set에 모두 적용한다.\"**\n",
    "- target 변수를 train, test set에 적용할 수 있는 이유는 **우리가 예측해야할 id는 date, bus_rout_id, station_name으로 구성되어있기 때문이다. 즉, 각각의 노선, 정류장별로 공통적인 패턴이 존재할 수 있다.**\n",
    "- 이 과정에서 NA 값이 생기는 이유는 train set에 없는 bus_route_id, station_name이 존재하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bus_route_id, station_name, weekday의 각 조합별 퇴근시간 하차인원의 여러 통계량 구하기\n",
    "# 종속변수 이용해서 통계량 항목 더하기\n",
    "# 질문 - 통계량이 왜 유용한 변수가 되는것인가? -> 뒤에 혼잡도 등 특정변수활용가능이유?\n",
    "\n",
    "# 종속변수와 - 특정 변수들(bus_route_id, station_name, weekday)간의 조합별 통계량을 구함 \n",
    "def id_statistic(ID, col1, col2) :\n",
    "    \n",
    "    # mean, sum\n",
    "    rs_mean = train.groupby([ID])['18~20_ride'].agg([(col1, 'mean')]).reset_index()\n",
    "    rs_sum = train.groupby([ID])['18~20_ride'].agg([(col2, 'sum')]).reset_index()\n",
    "    rs_mean_sum = pd.merge(rs_mean, rs_sum, on=ID)\n",
    "\n",
    "    # merge\n",
    "    tr = pd.merge(train, rs_mean_sum, how='left', on=ID)\n",
    "    te = pd.merge(test, rs_mean_sum, how='left', on=ID)\n",
    "\n",
    "    # na -> mean -> 널값 평균으로 채우기\n",
    "    te[col1] = te[col1].fillna(rs_mean.mean())\n",
    "    te[col2] = te[col2].fillna(rs_sum.mean())\n",
    "    \n",
    "    return tr, te\n",
    "\n",
    "# route_station과 종속변수에 대한 mean과 sum\n",
    "train, test = id_statistic('route_station', '1820_rs_mean', '1820_rs_sum')\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# bus_route_id 에 종속변수에 대한 mean과 sum\n",
    "train, test = id_statistic('bus_route_id', '1820_r_mean', '1820_r_sum')\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# station_code에 종속변수에 대한 mean과 sum\n",
    "train, test = id_statistic('station_code', '1820_s_mean', '1820_s_sum')\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# weekday에 대한 종속변수에 mean과 sum\n",
    "train, test = id_statistic('weekday', '1820_w_mean', '1820_w_sum')\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# 파생변수 3가지 bus_route_id_weekday / station_code_weekday / route_station_weekday 에 대해 평균구해서 항목 더하기\n",
    "def mean_statistics() :\n",
    "\n",
    "    f = train.groupby(['bus_route_id_weekday'])['18~20_ride'].agg([('mean_bus_weekday_ride','mean')]).reset_index()\n",
    "    tr = pd.merge(train, f, how='left', on='bus_route_id_weekday')\n",
    "    te = pd.merge(test, f, how='left', on='bus_route_id_weekday').fillna(f['mean_bus_weekday_ride'].mean())\n",
    "    \n",
    "    f = train.groupby(['station_code_weekday'])['18~20_ride'].agg([('mean_station_weekday_ride','mean')]).reset_index()\n",
    "    tr = pd.merge(tr, f, how='left', on='station_code_weekday')\n",
    "    te = pd.merge(te, f, how='left', on='station_code_weekday').fillna(f['mean_station_weekday_ride'].mean())\n",
    "    \n",
    "    f = train.groupby(['route_station_weekday'])['18~20_ride'].agg([('mean_route_station_weekday_ride','mean')]).reset_index()\n",
    "    tr = pd.merge(tr, f, how='left', on='route_station_weekday')\n",
    "    te = pd.merge(te, f, how='left', on='route_station_weekday').fillna(f['mean_route_station_weekday_ride'].mean())\n",
    "    \n",
    "    return tr, te\n",
    "\n",
    "train, test = mean_statistics()\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 혼잡도 계산하기\n",
    "def congestion() :\n",
    "    df = train.groupby(['bus_route_id'])['18~20_ride'].agg([('passenger', 'sum')])\n",
    "    df = df.sort_values(by='passenger', ascending=False).reset_index()\n",
    "    \n",
    "    # 승객수에 따라 혼잡도 등급 결정\n",
    "    def f(x):\n",
    "        if x > 10000:\n",
    "            return 7\n",
    "\n",
    "        elif x > 5000:\n",
    "            return 6\n",
    "\n",
    "        elif x > 2000:\n",
    "            return 5\n",
    "\n",
    "        elif x > 700:\n",
    "            return 4\n",
    "\n",
    "        elif x > 200:\n",
    "            return 3\n",
    "\n",
    "        elif x > 50:\n",
    "            return 2\n",
    "\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    # 버스 승객수에 따라서 혼잡도 등급 결정\n",
    "    df['congestion'] = df['passenger'].apply(f)\n",
    "    df = df[['bus_route_id','congestion']]\n",
    "    \n",
    "    tr = pd.merge(train, df, how='left', on='bus_route_id')\n",
    "    te = pd.merge(test, df, how='left', on='bus_route_id')\n",
    "    \n",
    "    # 결측치는 데이터 프레임 df의 'congestion'의 중간값인 '4'으로 대체\n",
    "    te = te.fillna(4)\n",
    "    \n",
    "    return tr, te\n",
    "train, test = congestion()\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location = latitude + longitude\n",
    "train['location'] = train['latitude'].astype(str) + ',' + train['longitude'].astype(str)\n",
    "test['location'] = test['latitude'].astype(str) + ',' + test['longitude'].astype(str)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge key\n",
    "# make cue column\n",
    "# cue는 train,test column 구분하려고\n",
    "train['cue']=0\n",
    "test['cue']=1\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오전 시간의 여러 데이터 활용한 변수\n",
    "# 오전 시간의 탑승 및 하차 데이터를 활용하여 요약통계량을 만든다.\n",
    "def morning() :\n",
    "    \n",
    "    # merge\n",
    "    data = pd.concat([train, test])\n",
    "    \n",
    "    # route_station (버스경로별 정거장) -> 10-12 승차인원의 합과 평균\n",
    "    a = data.groupby(['route_station'])['1012a'].agg({'sum', 'mean'}).reset_index()\n",
    "    a.columns = ['route_station', '1012a_sum','1012a_mean']\n",
    "\n",
    "    # route_station (버스경로별 정거장) -> 10-12 하차인원의 합과 평균\n",
    "    b = data.groupby(['route_station'])['1012b'].agg({'sum', 'mean'}).reset_index()\n",
    "    b.columns = ['route_station', '1012b_sum','1012b_mean']\n",
    "    b = b[['1012b_sum','1012b_mean']]\n",
    "\n",
    "    # route_station (버스경로별 정거장) -> 10-11 승차인원의 합과 평균\n",
    "    c = data.groupby(['route_station'])['10~11_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    c.columns = ['route_station', '10~11_ride_sum','10~11_ride_mean']\n",
    "    c = c[['10~11_ride_sum','10~11_ride_mean']]\n",
    "\n",
    "    # route_station (버스경로별 정거장) -> 10-11 하차인원의 합과 평균\n",
    "    d = data.groupby(['route_station'])['10~11_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    d.columns = ['route_station', '10~11_takeoff_sum','10~11_takeoff_mean']\n",
    "    d = d[['10~11_takeoff_sum','10~11_takeoff_mean']]\n",
    "\n",
    "    # route_station (버스경로별 정거장) -> 11-12 승차인원의 합과 평균\n",
    "    e = data.groupby(['route_station'])['11~12_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    e.columns = ['route_station', '11~12_ride_sum','11~12_ride_mean']\n",
    "    e = e[['11~12_ride_sum','11~12_ride_mean']]\n",
    "\n",
    "    # route_station (버스경로별 정거장) -> 11-12 하차인원의 합과 평균\n",
    "    f = data.groupby(['route_station'])['11~12_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    f.columns = ['route_station', '11~12_takeoff_sum','11~12_takeoff_mean']\n",
    "    f = f[['11~12_takeoff_sum','11~12_takeoff_mean']]\n",
    "\n",
    "    # route_station (버스경로id 와 종속변수(1820)과의 관계 평균) -> 합과 평균\n",
    "    g = data.groupby(['route_station'])['1820_r_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    g.columns = ['route_station', '1820_r_mean_sum','1820_r_mean_mean']\n",
    "    g = g[['1820_r_mean_sum','1820_r_mean_mean']]\n",
    "\n",
    "    # route_station (버스경로id 와 종속변수(1820)과의 관계 합) -> 합과 평균\n",
    "    h = data.groupby(['route_station'])['1820_r_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    h.columns = ['route_station', '1820_r_sum_sum','1820_r_sum_mean']\n",
    "    h = h[['1820_r_sum_sum','1820_r_sum_mean']]\n",
    "\n",
    "    # route_station (버스경로id-정거장code와 종속변수(1820)과의 관계 평균) -> 합과 평균\n",
    "    i = data.groupby(['route_station'])['1820_rs_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    i.columns = ['route_station', '1820_rs_mean_sum','1820_rs_mean_mean']\n",
    "    i = i[['1820_rs_mean_sum','1820_rs_mean_mean']]\n",
    "\n",
    "    # route_station (버스경로id-정거장code와 종속변수(1820)과의 관계 합) -> 합과 평균\n",
    "    j = data.groupby(['route_station'])['1820_rs_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    j.columns = ['route_station', '1820_rs_sum_sum','1820_rs_sum_mean']\n",
    "    j = j[['1820_rs_sum_sum','1820_rs_sum_mean']]\n",
    "\n",
    "    # route_station (정거장code와 종속변수(1820)과의 관계 평균) -> 합과 평균\n",
    "    k = data.groupby(['route_station'])['1820_s_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    k.columns = ['route_station', '1820_s_mean_sum','1820_s_mean_mean']\n",
    "    k = k[['1820_s_mean_sum','1820_s_mean_mean']]\n",
    "\n",
    "    # route_station (정거장code와 종속변수(1820)과의 관계 합) -> 합과 평균\n",
    "    l = data.groupby(['route_station'])['1820_s_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    l.columns = ['route_station', '1820_s_sum_sum','1820_s_sum_mean']\n",
    "    l = l[['1820_s_sum_sum','1820_s_sum_mean']]\n",
    "\n",
    "    # route_station (요일과 종속변수(1820)과의 관계 평균) -> 합과 평균    \n",
    "    m = data.groupby(['route_station'])['1820_w_mean'].agg({'sum', 'mean'}).reset_index()\n",
    "    m.columns = ['route_station', '1820_w_mean_sum','1820_w_mean_mean']\n",
    "    m = m[['1820_w_mean_sum','1820_w_mean_mean']]\n",
    "\n",
    "    # route_station (요일과 종속변수(1820)과의 관계 합) -> 합과 평균    \n",
    "    n = data.groupby(['route_station'])['1820_w_sum'].agg({'sum', 'mean'}).reset_index()\n",
    "    n.columns = ['route_station', '1820_w_sum_sum','1820_w_sum_mean']\n",
    "    n = n[['1820_w_sum_sum','1820_w_sum_mean']]\n",
    "\n",
    "    # route_station (6시-8시 승차인원의) -> 합과 평균        \n",
    "    o = data.groupby(['route_station'])['68a'].agg({'sum', 'mean'}).reset_index()\n",
    "    o.columns = ['route_station', '68a_sum','68a_mean']\n",
    "    o = o[['68a_sum','68a_mean']]\n",
    "\n",
    "    # route_station (6시-8시 하차인원의) -> 합과 평균        \n",
    "    p = data.groupby(['route_station'])['68b'].agg({'sum', 'mean'}).reset_index()\n",
    "    p.columns = ['route_station', '68b_sum','68b_mean']\n",
    "    p = p[['68b_sum','68b_mean']]\n",
    "\n",
    "    # route_station (6시-7시 승차인원의) -> 합과 평균        \n",
    "    q = data.groupby(['route_station'])['6~7_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    q.columns = ['route_station', '6~7_ride_sum','6~7_ride_mean']\n",
    "    q = q[['6~7_ride_sum','6~7_ride_mean']]\n",
    "\n",
    "    # route_station (6시-7시 하차인원의) -> 합과 평균        \n",
    "    r = data.groupby(['route_station'])['6~7_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    r.columns = ['route_station', '6~7_takeoff_sum','6~7_takeoff_mean']\n",
    "    r = r[['6~7_takeoff_sum','6~7_takeoff_mean']]\n",
    "\n",
    "    # route_station (7시-8시 승차인원의) -> 합과 평균            \n",
    "    s = data.groupby(['route_station'])['7~8_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    s.columns = ['route_station', '7~8_ride_sum','7~8_ride_mean']\n",
    "    s = s[['7~8_ride_sum','7~8_ride_mean']]\n",
    "\n",
    "    # route_station (7시-8시 하차인원의) -> 합과 평균            \n",
    "    t = data.groupby(['route_station'])['7~8_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    t.columns = ['route_station', '7~8_takeoff_sum','7~8_takeoff_mean']\n",
    "    t = t[['7~8_takeoff_sum','7~8_takeoff_mean']]\n",
    "\n",
    "    # route_station (8시-10시 승차인원의) -> 합과 평균            \n",
    "    u = data.groupby(['route_station'])['810a'].agg({'sum', 'mean'}).reset_index()\n",
    "    u.columns = ['route_station', '810a_sum','810a_mean']\n",
    "    u = u[['810a_sum','810a_mean']]\n",
    "\n",
    "    # route_station (8시-10시 하차인원의) -> 합과 평균            \n",
    "    v = data.groupby(['route_station'])['810b'].agg({'sum', 'mean'}).reset_index()\n",
    "    v.columns = ['route_station', '810b_sum','810b_mean']\n",
    "    v = v[['810b_sum','810b_mean']]\n",
    "\n",
    "    # route_station (8시-9시 승차인원의) -> 합과 평균            \n",
    "    w = data.groupby(['route_station'])['8~9_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    w.columns = ['route_station', '8~9_ride_sum','8~9_ride_mean']\n",
    "    w = w[['8~9_ride_sum','8~9_ride_mean']]\n",
    "\n",
    "    # route_station (8시-9시 하차인원의) -> 합과 평균            \n",
    "    x = data.groupby(['route_station'])['8~9_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    x.columns = ['route_station', '8~9_takeoff_sum','8~9_takeoff_mean']\n",
    "    x = x[['8~9_takeoff_sum','8~9_takeoff_mean']]\n",
    "\n",
    "    # route_station (9시-10시 승차인원의) -> 합과 평균            \n",
    "    y = data.groupby(['route_station'])['9~10_ride'].agg({'sum', 'mean'}).reset_index()\n",
    "    y.columns = ['route_station', '9~10_ride_sum','9~10_ride_mean']\n",
    "    y = y[['9~10_ride_sum','9~10_ride_mean']]\n",
    "\n",
    "    # route_station (9시-10시 하차인원의) -> 합과 평균            \n",
    "    z = data.groupby(['route_station'])['9~10_takeoff'].agg({'sum', 'mean'}).reset_index()\n",
    "    z.columns = ['route_station', '9~10_takeoff_sum','9~10_takeoff_mean']\n",
    "    z = z[['9~10_takeoff_sum','9~10_takeoff_mean']]\n",
    "    \n",
    "    df = pd.concat([a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z],axis=1)\n",
    "    df = pd.merge(data, df, how='left', on='route_station')\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = morning()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배차간격 (시간 많이 소요됨)\n",
    "train['bus_route_id'] = train['bus_route_id'].astype(np.int64)\n",
    "test['bus_route_id'] = test['bus_route_id'].astype(np.int64)\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bts.shape)\n",
    "bts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bts\n",
    "# datetime 형식으로 변환 -> 오늘날짜 + 해당 geton time -> ex) 2020-05-02 06:34:45\n",
    "bts['geton_time2'] = pd.to_datetime(bts['geton_time'])\n",
    "\n",
    "f = bts.groupby(['geton_date','geton_time2','geton_station_code','bus_route_id'])['user_count']\\\n",
    "    .agg([('탑승객_수','sum')]).reset_index()\\\n",
    "    .sort_values(by=['geton_date','geton_station_code','bus_route_id','geton_time2'], ascending=True).reset_index()\n",
    "\n",
    "f['index'] = list(range(0,len(f)))\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간상 bus_interval_data 제공\n",
    "'''\n",
    "# 시간 굉장히 오래걸림\n",
    "# 약 1시간\n",
    "time = []\n",
    "print(len(f))\n",
    "for i in range(0,len(f)-1):\n",
    "    if ((f.iloc[i].geton_date == f.iloc[i+1].geton_date) &\\\n",
    "        (f.iloc[i].geton_station_code == f.iloc[i+1].geton_station_code) &\\\n",
    "        (f.iloc[i].bus_route_id == f.iloc[i+1].bus_route_id)):\n",
    "        \n",
    "        time.append(f.iloc[i+1].geton_time2 - f.iloc[i].geton_time2)\n",
    "    else:\n",
    "        time.append(0)\n",
    "\n",
    "time.insert(0, '0')\n",
    "time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간상 bus_interval_data 제공\n",
    "'''\n",
    "def get_sec(time_str):\n",
    "    h, m, s = time_str.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간상 bus_interval_data 제공\n",
    "'''\n",
    "def bus_interval() :\n",
    "    \n",
    "    f['time'] = time\n",
    "    f['time2'] = f['time'].astype(str).str[7:]\n",
    "\n",
    "    interval = f.copy()\n",
    "    interval['time2'] = interval['time2'].astype(str).replace('','00:00:00')\n",
    "    interval['bus_route_id'] = interval['bus_route_id'].astype(object)\n",
    "\n",
    "    time4 = []\n",
    "\n",
    "    for i in interval['time2'] :\n",
    "\n",
    "        time4.append(get_sec(i))\n",
    "\n",
    "    interval['time4'] = time4\n",
    "    interval['time4'] = (interval['time4'] / 60).astype(int)\n",
    "\n",
    "    interval = interval[interval['time4'] > 3] # 간격이 3분보다 작은 것 제외 \n",
    "    interval = interval[interval['time4'] < 180] # 간격이 3시간보다 큰 것 제외\n",
    "\n",
    "    interval = interval.groupby('bus_route_id')['time4'].agg([('bus_interval', 'mean')]).reset_index()\n",
    "    interval['bus_interval'] = interval['bus_interval'].astype(int)\n",
    "\n",
    "    # 나중에 시간을 절약하기 위해 csv 파일로 저장\n",
    "    interval.to_csv('bus_interval_final.csv', index = False)\n",
    "\n",
    "    print('success.. !')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간절약을 위해 데이터 로딩 bus_interval_final\n",
    "bus_interval = pd.read_csv(\"bus_interval_final.csv\")\n",
    "print(bus_interval.shape)\n",
    "print(bus_interval.columns)\n",
    "bus_interval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['bus_route_id'] = data['bus_route_id'].astype(np.int64)\n",
    "data = pd.merge(data, bus_interval, how = 'left', on = 'bus_route_id')\n",
    "print('완료', data.shape)\n",
    "print(data.columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'bus_interval'컬럼에서 na값을 가지는 행들을 분석해본 결과, \n",
    "# 대부분 18~20시의 탑승 인원이 거의 없는 bus_route_id 와 station_code 였다.\n",
    "# 따라서 탑승 인원이 별로 없을 것이라고 예상되는 버스는 배차 간격이 길 것이라고 판단하여\n",
    "# na값을 '9999' 로 채워주었다.\n",
    "data['bus_interval'] = data['bus_interval'].fillna(9999)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding feature\n",
    "- bus_route_id, station_name, route_station_weekday(bus_route_id + weekday의 조합), route_station(bus_route_id + station_code 의 조합) 총 4개를 라벨인코딩 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df_encode = data[['bus_route_id','station_code', 'route_station_weekday', 'route_station']]\n",
    "df_encoded = df_encode.apply(labelencoder.fit_transform)\n",
    "\n",
    "data['bus_route_id2']=df_encoded['bus_route_id']\n",
    "data['station_code2']=df_encoded['station_code']\n",
    "data['route_station_weekday2']=df_encoded['route_station_weekday']\n",
    "data['route_station2']=df_encoded['route_station']\n",
    "\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather - 날씨\n",
    "def weather() :\n",
    "    weather_data = pd.read_csv('weather.csv', encoding = 'euc-kr')\n",
    "    weather_data['id'] = range(0,46)\n",
    "\n",
    "    a = pd.DataFrame(data.date.unique(), columns=['date']) ; a['id'] = range(0,46)\n",
    "    weather_data = pd.merge(a, weather_data)\n",
    "    weather_data = weather_data[['date','현재일기_10']]#,'체감온도_10','일강수_10']]\n",
    "    weather_data = weather_data.replace(' ', 0)\n",
    "    df = pd.merge(data, weather_data, on='date')\n",
    "    \n",
    "    # label_encoder\n",
    "    labelencoder = LabelEncoder()\n",
    "    df_encode = df[['현재일기_10']]\n",
    "    df_encoded = df_encode.apply(labelencoder.fit_transform)\n",
    "    df['현재일기_10'] = df_encoded['현재일기_10'] # object 형\n",
    "    df['현재일기_10'] = df['현재일기_10'].astype(float) # float 형 변환\n",
    "#    df['체감온도_10'] = df['체감온도_10'].astype(float)\n",
    "#    df['일강수_10'] = df['일강수_10'].astype(float)\n",
    "    return df\n",
    "\n",
    "data = weather()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday(data)\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data = pd.get_dummies(data,columns=['weekday'])\n",
    "data['weekday'] = data['date'].dt.weekday\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-out\n",
    "data['in_out'].value_counts()\n",
    "data['in_out'] = data['in_out'].map({'시내':0,'시외':1})\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-6) 제주특별자치도의 시별 탑승객 수\n",
    "#### 분석결과 \n",
    "- 제주시의 평균 탑승객 수와 서귀포시의 평균 탑승객 수는 거의 두 배 차이임.\n",
    "- 따라서 제주시에 가까운 버스정류장일수록 탑승객이 많을 것이며, \n",
    "- 서귀포시에 가까운 버스정류장일수록 탑승객이 적을 것으로 예상됨.\n",
    "\n",
    "#### 피쳐 생성\n",
    "- dis_jejusi : 위도 경도 정보를 통해 제주시의 중심으로부터 거리 정보가 담겨 있음.\n",
    "- si : 외부 프로그램을 사용하여 주소를 추출한 후 제주시와 서귀포시 더미 변수 생성\n",
    "\n",
    "![EDA_1-7](./img/EDA_1-7.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium # 지도 관련 시각화\n",
    "from folium.plugins import MarkerCluster #지도 관련 시각화\n",
    "import geopy.distance #거리 계산해주는 패키지 사용\n",
    "\n",
    "# 좌표데이터를 이용 변수\n",
    "# 좌표데이터를 이용한 변수를 만들 때는 많은 시간이 소요됩니다.\n",
    "coords_jejusi = (33.500770, 126.522761) #제주시의 위도 경도\n",
    "data['dis_jejusi'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejusi).km for i in range(len(data))]\n",
    "\n",
    "coords_jejusicheong1 = (33.49892, 126.53035) #제주시청(광양방면)의 위도 경도\n",
    "coords_jejuairport = (33.50661, 126.49345) #제주국제공항(구제주방면)의 위도 경도\n",
    "coords_hallahosp = (33.48963, 126.486) #한라병원의 위도 경도\n",
    "coords_rotary = (33.49143, 126.49678) # 제주도청신제주로터리의 위도 경도\n",
    "coords_jejucenterhigh = (33.48902, 126.5392) #제주중앙여자고등학교의 위도 경도\n",
    "coords_jejumarket = (33.51315, 126.52706) #동문시장의 위도 경도\n",
    "coords_jejusclass = (33.47626, 126.48141) #제주고등학교/중흥S클래스의 위도 경도\n",
    "coords_centerroad = (33.51073, 126.5239) #중앙로(국민은행)의 위도 경도\n",
    "coords_fiveway = (33.48667, 126.48092) # 노형오거리의 위도 경도\n",
    "coords_law = (33.49363, 126.53476) # 제주지방법원(광양방면)의 위도 경도\n",
    "\n",
    "data['dis_jejusicheong1'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejusicheong1).km for i in range(len(data))]\n",
    "print('dis_jejusicheong1 완료')\n",
    "\n",
    "data['dis_jejuairport'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejuairport).km for i in range(len(data))]\n",
    "print('dis_jejuairport 완료')\n",
    "\n",
    "data['dis_hallahosp'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_hallahosp).km for i in range(len(data))]\n",
    "print('dis_hallahosp 완료')\n",
    "\n",
    "data['dis_rotary'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_rotary).km for i in range(len(data))]\n",
    "print('dis_rotary 완료')\n",
    "\n",
    "data['dis_jejucenterhigh'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejucenterhigh).km for i in range(len(data))]\n",
    "print('dis_jejucenterhigh 완료')\n",
    "\n",
    "data['dis_jejumarket'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejumarket).km for i in range(len(data))]\n",
    "print('dis_jejumarket 완료')\n",
    "\n",
    "data['dis_jejusclass'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_jejusclass).km for i in range(len(data))]\n",
    "print('dis_jejusclass 완료')\n",
    "\n",
    "data['dis_centerroad'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_centerroad).km for i in range(len(data))]\n",
    "print('dis_centerroad 완료')\n",
    "\n",
    "data['dis_fiveway'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_fiveway).km for i in range(len(data))]\n",
    "print('dis_fiveway 완료')\n",
    "\n",
    "data['dis_law'] = [geopy.distance.vincenty((data['latitude'].iloc[i],data['longitude'].iloc[i]), coords_law).km for i in range(len(data))]\n",
    "print('dis_law 완료')\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출근 시간의 총 승객 수\n",
    "data['ride_sum'] = data['6~7_ride'] + data['7~8_ride'] + data['8~9_ride'] + data['9~10_ride'] + data['10~11_ride'] + data['11~12_ride'] \n",
    "data['takeoff_sum'] = data['6~7_takeoff'] + data['7~8_takeoff'] + data['8~9_takeoff'] + data['9~10_takeoff'] + data['10~11_takeoff'] + data['11~12_takeoff'] \n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 및 시간대 별 총 승객수\n",
    "f = data.groupby('date')['6~7_ride'].agg([('6~7_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['7~8_ride'].agg([('7~8_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['8~9_ride'].agg([('8~9_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['9~10_ride'].agg([('9~10_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "f = data.groupby('date')['10~11_ride'].agg([('10~11_all_ride_number', 'sum')]).reset_index()\n",
    "data = pd.merge(data, f, how='left')\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주말, 주중\n",
    "def h(x):\n",
    "    if x ==5:\n",
    "        return 1\n",
    "    elif x==6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['weekend'] = data['weekday'].apply(h)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연휴\n",
    "def g(x):\n",
    "    if x in ['2019-09-12','2019-09-13','2019-09-14','2019-10-03','2019-10-09']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['holiday'] = data['date'].apply(g) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요일 별 평균 승객 수\n",
    "def week_mean() :\n",
    "\n",
    "    df = data.reset_index(drop=True)\n",
    "    df.groupby('weekday')['18~20_ride'].mean()\n",
    "    df['weekdaymean']= 1\n",
    "\n",
    "    index0 = df.query('weekday==0').index\n",
    "    index1 = df.query('weekday==1').index\n",
    "    index2 = df.query('weekday==2').index\n",
    "    index3 = df.query('weekday==3').index\n",
    "    index4 = df.query('weekday==4').index\n",
    "    index5 = df.query('weekday==5').index\n",
    "    index6 = df.query('weekday==6').index\n",
    "\n",
    "    df.iloc[index0,-1] = 1.343710\n",
    "    df.iloc[index1,-1] = 1.375319\n",
    "    df.iloc[index2,-1] = 1.430856\n",
    "    df.iloc[index3,-1] = 1.256710\n",
    "    df.iloc[index4,-1] = 1.067439\n",
    "    df.iloc[index5,-1] = 1.062123\n",
    "    df.iloc[index6,-1] = 1.034282\n",
    "\n",
    "    return df\n",
    "data = week_mean()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시내 및 시외버스 별 평균 탑승 승객\n",
    "data['in_out_mean'] = 1\n",
    "inindex = data.query('in_out == \"시내\"').index\n",
    "outindex = data.query('in_out == \"시외\"').index\n",
    "\n",
    "data.iloc[inindex,-1] = 1.228499\n",
    "data.iloc[outindex,-1] = 2.044345\n",
    "data['congestion'] = data['congestion'].astype('int64')\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리별 승객 수\n",
    "def category_people() :\n",
    "    bts['bus_route_id'] = bts['bus_route_id'].astype(str)\n",
    "\n",
    "    f = bts.groupby(['bus_route_id','user_category'])['user_count'].agg([('승객수', 'sum')]).reset_index()\n",
    "\n",
    "    g = pd.pivot_table(f, values='승객수', index='bus_route_id', columns='user_category',fill_value=0).reset_index()\n",
    "    g.columns = ['bus_route_id', 'adult','kids','teen','elder','jang','jang2','ugong','ugong2']\n",
    "    g = g[['bus_route_id', 'adult','kids','teen','elder']]\n",
    "\n",
    "    # merge\n",
    "    df = pd.merge(data, g, how='left', on='bus_route_id')\n",
    "\n",
    "    # na preprocessing -&gt; mean value\n",
    "    df['adult'] = df['adult'].fillna(2363.077778)\n",
    "    df['kids'] = df['kids'].fillna(60.426984)\n",
    "    df['teen'] = df['teen'].fillna(448.277778)\n",
    "    df['elder'] = df['elder'].fillna(751.309524)\n",
    "                 \n",
    "    return df\n",
    "data['bus_route_id'] = data['bus_route_id'].astype(str)\n",
    "data = category_people()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category별 승객의 비율\n",
    "def category_people_ratio() :\n",
    "    \n",
    "    a = bts.groupby('bus_route_id')['user_count'].agg([('전체', 'sum')]).reset_index()\n",
    "    b = bts.groupby(['bus_route_id','user_category'])['user_count'].agg([('승객수', 'sum')]).reset_index()\n",
    "\n",
    "    c = pd.merge(b, a, on='bus_route_id')\n",
    "    c['비율'] = c['승객수']/c['전체']\n",
    "    c = pd.pivot_table(c, values='비율', index='bus_route_id', columns='user_category',fill_value=0).reset_index()\n",
    "    c.columns = ['bus_route_id', 'adult_prop','kids_prop','teen_prop','elder_prop','jang_prop','jang2_prop','ugong_prop','ugong2_prop']\n",
    "    f = c[['bus_route_id', 'adult_prop','kids_prop','teen_prop','elder_prop']]\n",
    "\n",
    "    df = pd.merge(data, f, how='left', on='bus_route_id')\n",
    "\n",
    "    # na preprocessing -&gt; mean value\n",
    "    df['adult_prop'] = df['adult_prop'].fillna(0.549702)\n",
    "    df['kids_prop'] = df['kids_prop'].fillna(60.426984)\n",
    "    df['teen_prop'] = df['teen_prop'].fillna(0.019902)\n",
    "    df['elder_prop'] = df['elder_prop'].fillna(0.235848)\n",
    "\n",
    "    return df\n",
    "data = category_people_ratio()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-7) 소비 비율에 따른 18시~20시 탑승객 수\n",
    "#### 분석결과\n",
    "- 평균 소비액이 높다는 것은 그만큼 소비자, 즉 유동인구가 많다는 것을 의미하기도 함.\n",
    "- 아래 그래프는 평균 소비액이 높을수록 대체로 탑승객 수가 많음을 보여줌.\n",
    "\n",
    "#### 피쳐 생성\n",
    "- mean_avg_spend : 동별 소비액의 평균\n",
    "- sum_avg_spend : 동별 소비액의 합\n",
    "- rate_avg_spend : 동별 소비액의 비율\n",
    "![EDA_1-6](./img/EDA_1-6.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 동,읍,면별 직업군별 비율, 평균소득액, 평균소비액\n",
    "# jeju_finantial_life_data (외부데이터) 사용하여 피쳐 생성\n",
    "\n",
    "# train, test 데이터의 'latitude', 'longitude'열, jeju_financial_life_data의 'X_AXIS', 'Y_AXIS'열을 지오코딩 프로그램을 사용하여 주소로 변환하였다.\n",
    "# 변환한 주소를 전처리 하여 동 기준으로 두 데이터를 합쳐주었다. 그 뒤 파생변수를 생성하였다. \n",
    "# data_address.csv는 data의 주소가, life_address.csv는 jeju_finantial_life_data의 주소가 담겨있다.\n",
    "\n",
    "def jeoju_love() :\n",
    "    loc_data = pd.read_csv(\"data_address.csv\", encoding='cp949')\n",
    "    loc_life = pd.read_csv(\"life_address.csv\", encoding='cp949')\n",
    "    \n",
    "    loc_data = loc_data[['location','dong', 'si']]\n",
    "    loc_life = loc_life[['location','dong', 'si']]\n",
    "\n",
    "    df = pd.merge(data, loc_data, how='left', on='location')\n",
    "    \n",
    "    jeju_life = pd.read_csv(\"jeju_financial_life_data.csv\")\n",
    "    jeju_life['location'] = jeju_life['x_axis'].astype(str).str[:10] + ',' + jeju_life['y_axis'].astype(str).str[:10]\n",
    "    jeju_life2 = pd.merge(jeju_life, loc_life, how='left', on='location')\n",
    "\n",
    "    dong_f1 = jeju_life2.groupby(['dong'])[['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend']].mean().reset_index()\n",
    "    dong_f1.columns=['dong','mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self','mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend']\n",
    "\n",
    "    dong_f2 = jeju_life2.groupby(['dong'])[['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend']].sum().reset_index()\n",
    "    dong_f2.columns=['dong','sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self','sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income', 'sum_avg_spend']\n",
    "\n",
    "    dong_f3 = (jeju_life2.groupby(['dong'])['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend'].sum()/jeju_life2.groupby(['dong'])['job_majorc', 'job_smallc', 'job_public', 'job_profession', 'job_self','vehicle_own_rat', 'avg_income', 'med_income', 'avg_spend'].sum().sum()).reset_index()\n",
    "    dong_f3.columns = ['dong','rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self','rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income', 'rate_avg_spend']\n",
    "\n",
    "    m_1 = pd.merge(dong_f1, dong_f2, how='left', on='dong')\n",
    "    m_2 = pd.merge(m_1, dong_f3, how='left', on='dong')\n",
    "    df = pd.merge(df, m_2, how='left', on='dong')\n",
    "\n",
    "    #  # na preprocessing -&gt; mean value\n",
    "    df['mean_job_majorc'] = df['mean_job_majorc'].fillna(0.024219)\n",
    "    df['mean_job_smallc'] = df['mean_job_smallc'].fillna(0.145757)\n",
    "    df['mean_job_public'] = df['mean_job_public'].fillna(0.032768)\n",
    "    df['mean_job_profession'] = df['mean_job_profession'].fillna(0.014855)\n",
    "    df['mean_job_self'] = df['mean_job_self'].fillna(0.222090)\n",
    "    df['mean_vehicle_own_rat'] = df['mean_vehicle_own_rat'].fillna(0.041161)\n",
    "    df['mean_avg_income'] = df['mean_avg_income'].fillna(34221420)\n",
    "    df['mean_med_income'] = df['mean_med_income'].fillna(30645290)\n",
    "    df['mean_avg_spend'] = df['mean_avg_spend'].fillna(4224923)\n",
    "\n",
    "    df['sum_job_majorc'] = df['sum_job_majorc'].fillna(3.717861e+00)\n",
    "    df['sum_job_smallc'] = df['sum_job_smallc'].fillna(2.078142e+01)\n",
    "    df['sum_job_public'] = df['sum_job_public'].fillna(4.747755e+00)\n",
    "    df['sum_job_profession'] = df['sum_job_profession'].fillna(2.169554e+00)\n",
    "    df['sum_job_self'] = df['sum_job_self'].fillna(3.044199e+01)\n",
    "    df['sum_vehicle_own_rat'] = df['sum_vehicle_own_rat'].fillna(5.609080e+00)\n",
    "    df['sum_avg_income'] = df['sum_avg_income'].fillna(4.998226e+09)\n",
    "    df['sum_med_income'] = df['sum_med_income'].fillna(4.455924e+09)\n",
    "    df['sum_avg_spend'] = df['sum_avg_spend'].fillna(6.147678e+08)\n",
    "\n",
    "    df['rate_job_majorc'] = df['rate_job_majorc'].fillna(1.388889e-02)\n",
    "    df['rate_job_smallc'] = df['rate_job_smallc'].fillna(1.388889e-02)\n",
    "    df['rate_job_public'] = df['rate_job_public'].fillna(1.388889e-02)\n",
    "    df['rate_job_profession'] = df['rate_job_profession'].fillna(1.388889e-02)\n",
    "    df['rate_job_self'] = df['rate_job_self'].fillna(1.388889e-02)\n",
    "    df['rate_vehicle_own_rat'] = df['rate_vehicle_own_rat'].fillna(1.388889e-02)\n",
    "    df['rate_avg_income'] = df['rate_avg_income'].fillna(1.388889e-02)\n",
    "    df['rate_med_income'] = df['rate_med_income'].fillna(1.388889e-02)\n",
    "    df['rate_avg_spend'] = df['rate_avg_spend'].fillna(1.388889e-02)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jeoju_love()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data[data['station_name'].str.contains('고등학교')]\n",
    "highschool = list(g['station_name'].unique())\n",
    "\n",
    "g = data[data['station_name'].str.contains('대학교')]\n",
    "university = list(g['station_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x in highschool:\n",
    "        return 1\n",
    "    elif x in university:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['school'] = data['station_name'].apply(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data[data['station_name'].str.contains('환승')]\n",
    "transfer = list(g['station_name'].unique())\n",
    "\n",
    "g = data[data['station_name'].str.contains('공항')]\n",
    "airport = list(g['station_name'].unique())\n",
    "\n",
    "g = data[data['station_name'].str.contains('터미널')]\n",
    "terminal = list(g['station_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x in transfer:\n",
    "        return 1\n",
    "    elif x in airport:\n",
    "        return 1\n",
    "    elif x in terminal:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['transfer'] = data['station_name'].apply(f) \n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동 라벨 인코딩\n",
    "labelencoder = LabelEncoder()\n",
    "df_encode = data[['dong']]\n",
    "df_encoded = df_encode.apply(labelencoder.fit_transform)\n",
    "\n",
    "data['dong2']=df_encoded['dong']\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 측정소와 정류장 사이의 거리 계산\n",
    "def dist() :\n",
    "    jeju=(33.51411, 126.52969) # 제주 측정소 근처\n",
    "    gosan=(33.29382, 126.16283) #고산 측정소 근처\n",
    "    seongsan=(33.38677, 126.8802) #성산 측정소 근처\n",
    "    po=(33.24616, 126.5653) #서귀포 측정소 근처\n",
    "\n",
    "    t1 = [geopy.distance.vincenty( (i,j), jeju).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "    t2 = [geopy.distance.vincenty( (i,j), gosan).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "    t3 = [geopy.distance.vincenty( (i,j), seongsan).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "    t4 = [geopy.distance.vincenty( (i,j), po).km for i,j in list( zip( data['latitude'],data['longitude'] )) ]\n",
    "\n",
    "    data['dis_jeju'] = t1\n",
    "    data['dis_gosan']=t2\n",
    "    data['dis_seongsan']=t3\n",
    "    data['dis_po']=t4\n",
    "\n",
    "    total = pd.DataFrame(list(zip( t1,t2,t3,t4)),columns=['jeju','gosan','seongsan','po'] )\n",
    "    data['dist_name'] = total.apply(lambda x: x.argmin(), axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dist()\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨 관련 변수\n",
    "# 중급자코드를 참고하면 rain3.csv를 만들 수 있다.\n",
    "\n",
    "rain3 = pd.read_csv(\"rain3.csv\")\n",
    "\n",
    "# train, test의 변수명과 통일시키고, NaN의 값은 0.0000으로 변경\n",
    "rain3 = rain3.rename(columns={\"일시\":\"date\",\"지점\":\"dist_name\"})\n",
    "rain3 = rain3.fillna(0.00000)\n",
    "rain3['date'] = pd.to_datetime(rain3['date'])\n",
    "\n",
    "rain3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, rain3, how='left',on=['dist_name','date'])\n",
    "data = pd.get_dummies(data,columns=['dist_name'])\n",
    "data = pd.get_dummies(data,columns=['si'])\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rainy_day\n",
    "# 비 오는날 = 1, 비오는날 = 0\n",
    "def f(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "data['rainy_day'] = data['강수량(mm)'].apply(f)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 승 하차 시간대 통합 변수 (t ~ t+3)\n",
    "# t~t+1, t+1~t+2, t+2~t+3 시간대 승하차인원을 합하여 t~t+3 시간대 승하차인원 변수를 만듬\n",
    "data['69a'] = data['6~7_ride']+data['7~8_ride']+data['8~9_ride']\n",
    "data['912a']=data['9~10_ride']+data['10~11_ride']+data['11~12_ride']\n",
    "\n",
    "data['69b'] = data['6~7_takeoff']+data['7~8_takeoff']+data['8~9_takeoff']\n",
    "data['912b'] = data['9~10_takeoff']+data['10~11_takeoff']+data['11~12_takeoff']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "# train, test 데이터를 만들어준다.\n",
    "\n",
    "train_data = data.query('cue==\"0\"').reset_index()\n",
    "test_data = data.query('cue==\"1\"').reset_index()\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약통계량을 통한 변수를 만드는 과정에서 NA가 발생한다.\n",
    "# target variable을 분리한 후 데이터를 저장한다.\n",
    "y_train = train_data[['18~20_ride']]\n",
    "train_data.shape, test_data.shape, y_train.shape\n",
    "\n",
    "train_data.to_csv('X_train.csv', index = False, encoding = 'utf-8')\n",
    "test_data.to_csv('X_test.csv', index = False, encoding = 'utf-8')\n",
    "y_train.to_csv('y_train.csv', index = False, encoding = 'utf-8')\n",
    "\n",
    "print('save ..!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('X_train.csv', encoding = 'utf-8')\n",
    "test_data = pd.read_csv('X_test.csv', encoding = 'utf-8')\n",
    "y_train = pd.read_csv('y_train.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "input_var_0=['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', 'elder_prop',\n",
    "           'mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self',\n",
    "           'mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend', \n",
    "           'rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self', \n",
    "           'rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income','rate_avg_spend',\n",
    "           'sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self', \n",
    "           'sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income','sum_avg_spend',\n",
    "           '68a', '810a', '1012a', '68b', '810b', '1012b','69a','912a','69b','912b',\n",
    "           'dis_jeju', 'dis_gosan', 'dis_seongsan', 'dis_po', '기온(°C)', '강수량(mm)',\n",
    "           'dist_name_gosan', 'dist_name_jeju', 'dist_name_po', 'dist_name_seongsan', 'si_서귀포시', 'si_제주시',\n",
    "           'school', 'transfer', 'dong2', 'rainy_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_1=['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', 'elder_prop',\n",
    "           'mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self',\n",
    "           'mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend', \n",
    "           'rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self', \n",
    "           'rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income','rate_avg_spend',\n",
    "           'sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self', \n",
    "           'sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income','sum_avg_spend',\n",
    "           '68a', '810a', '1012a', '68b', '810b', '1012b','69a','912a','69b','912b',\n",
    "           'dis_jeju', 'dis_gosan', 'dis_seongsan', 'dis_po', '기온(°C)', '강수량(mm)',\n",
    "           'dist_name_gosan', 'dist_name_jeju', 'dist_name_po', 'dist_name_seongsan', 'si_서귀포시', 'si_제주시',\n",
    "           'school', 'transfer', 'dong2', 'rainy_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_2=['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', 'elder_prop',\n",
    "           'mean_job_majorc', 'mean_job_smallc', 'mean_job_public', 'mean_job_profession', 'mean_job_self',\n",
    "           'mean_vehicle_own_rat', 'mean_avg_income', 'mean_med_income', 'mean_avg_spend', \n",
    "           'rate_job_majorc', 'rate_job_smallc', 'rate_job_public', 'rate_job_profession', 'rate_job_self', \n",
    "           'rate_vehicle_own_rat', 'rate_avg_income', 'rate_med_income','rate_avg_spend',\n",
    "           'sum_job_majorc', 'sum_job_smallc', 'sum_job_public', 'sum_job_profession', 'sum_job_self', \n",
    "           'sum_vehicle_own_rat', 'sum_avg_income', 'sum_med_income','sum_avg_spend',\n",
    "           '68a', '810a', '1012a', '68b', '810b', '1012b','69a','912a','69b','912b',\n",
    "           'dis_jeju', 'dis_gosan', 'dis_seongsan', 'dis_po', '기온(°C)', '강수량(mm)',\n",
    "           'dist_name_gosan', 'dist_name_jeju', 'dist_name_po', 'dist_name_seongsan', 'si_서귀포시', 'si_제주시',\n",
    "           'school', 'transfer', 'dong2', 'rainy_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_3 =['in_out','latitude','longitude','6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride',\n",
    "           '6~7_takeoff', '7~8_takeoff', '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_jejusicheong1','dis_jejuairport','dis_hallahosp', 'dis_rotary','dis_jejucenterhigh',\n",
    "           'dis_jejumarket', 'dis_centerroad', 'dis_jejusclass', 'dis_fiveway', 'dis_law',\n",
    "           'weekend', 'holiday', 'ride_sum', 'takeoff_sum', '1820_rs_mean', '1820_r_mean', '1820_s_mean', 'congestion',\n",
    "           'station_code2', 'bus_route_id2', '일강수_10', '현재일기_10', '체감온도_10',\n",
    "           '6~7_all_ride_number', '7~8_all_ride_number', '8~9_all_ride_number', '9~10_all_ride_number', '10~11_all_ride_number',\n",
    "           '1820_w_mean','in_out_mean','weekdaymean','adult','kids','teen','elder','adult_prop', 'kids_prop', 'teen_prop', \n",
    "           'elder_prop', 'mean_job_majorc', 'mean_job_smallc',\n",
    "           'mean_job_public', 'mean_job_profession', 'mean_job_self','mean_vehicle_own_rat',\n",
    "          '68a', '810a', '1012a', '68b', '810b', '1012b',\n",
    "          'dis_jeju', 'dis_gosan','dis_seongsan', 'dis_po','기온(°C)', '강수량(mm)', \n",
    "           'dist_name_gosan', 'dist_name_jeju','dist_name_po', 'dist_name_seongsan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var_4 = ['sum_avg_spend', '68a', '810a', 'si_제주시',  'dong2' , 'bus_interval', 'dis_jejuairport', 'ride_sum',\n",
    "             'takeoff_sum', '1820_rs_mean', '1820_rs_sum','1820_r_mean','1820_r_sum', '1820_s_mean',\n",
    "             '1820_s_sum','congestion', 'bus_route_id2', '6~7_all_ride_number', '7~8_all_ride_number',\n",
    "             '8~9_all_ride_number', '1012a_mean', '1012b_sum','10~11_ride_sum', '10~11_takeoff_sum', '11~12_ride_sum',\n",
    "             '11~12_takeoff_sum', '1820_r_mean_sum', '1820_r_mean_mean',\n",
    "             '1820_r_sum_sum', '1820_r_sum_mean', '1820_rs_mean_sum',\n",
    "             '1820_s_mean_sum', '1820_s_mean_mean', '1820_s_sum_sum',\n",
    "             '1820_s_sum_mean', '1820_w_mean_sum', '1820_w_mean_mean',\n",
    "             '1820_w_sum_mean', '68a_sum', '68a_mean', '68b_sum', 'in_out', 'latitude', 'longitude',\n",
    "             '6~7_ride', '7~8_ride', '8~9_ride', '9~10_ride', '10~11_ride', '11~12_ride', '6~7_takeoff', '7~8_takeoff',\n",
    "             '8~9_takeoff', '9~10_takeoff', '10~11_takeoff', '11~12_takeoff',\n",
    "             'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
    "             'weekday_5', 'weekday_6', 'dis_jejusi', '68b_mean', '6~7_ride_sum',\n",
    "             '6~7_ride_mean', '6~7_takeoff_sum', '6~7_takeoff_mean', '7~8_ride_sum',\n",
    "             '7~8_ride_mean', '7~8_takeoff_sum', '7~8_takeoff_mean', '810a_sum',\n",
    "             '810b_sum', '8~9_ride_sum', '8~9_takeoff_sum', '8~9_takeoff_mean',\n",
    "             '9~10_ride_sum', '9~10_takeoff_sum', 'route_station_weekday2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from keras import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "# 모델 학습 및 검증\n",
    "lgbm = lgb.LGBMRegressor(num_iterations = 1000, \n",
    "                                learning_rate = 0.05,\n",
    "                                boosting = 'dart',\n",
    "                         Metric = 'regression_l2', n_jobs=-1)\n",
    "\n",
    "X = train_data[input_var_0]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns=range(len(X.columns))\n",
    "lgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict(X)\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "y_pred = lgbm.predict(test_data[input_var_0])\n",
    "\n",
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('lgbm0=229.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task - 1\n",
    "rf = RandomForestRegressor(max_features=5,\n",
    "                           min_samples_leaf=1,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=300,\n",
    "                           random_state=1217)\n",
    "\n",
    "X = train_data[input_var_1]\n",
    "y = y_train\n",
    "rf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_1])\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "y_pred = rf.predict(test_data[input_var_1])\n",
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf1=234.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task - 2\n",
    "rf = RandomForestRegressor(random_state=1217,\n",
    "                           max_features= 3,\n",
    "                           min_samples_leaf= 2,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=500)\n",
    "\n",
    "X = train_data[input_var_2]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_2])\n",
    "np.sqrt(mean_squared_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_data[input_var_2])\n",
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf2=238.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task - 3\n",
    "rf = RandomForestRegressor(max_features=3,\n",
    "                           min_samples_leaf=2,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=500,\n",
    "                           random_state=1217)\n",
    "\n",
    "X = train_data[input_var_3]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_data[input_var_3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf3=236.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task - 4\n",
    "rf = RandomForestRegressor(max_features=3,\n",
    "                           min_samples_leaf=2,\n",
    "                           min_samples_split=2,\n",
    "                           n_estimators=500,\n",
    "                           random_state=1217)\n",
    "\n",
    "X = train_data[input_var_4]\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(train_data[input_var_4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_data[input_var_4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub['18~20_ride'] = y_pred\n",
    "sub.to_csv('rf4=231.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n",
    "# 변수별 성능 및 상관관계 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "file_list = ['lgbm0=229.csv', 'rf1=234.csv', 'rf2=238.csv', 'rf3=236.csv', 'rf4=231.csv']\n",
    "\n",
    "for item in file_list :\n",
    "    if item.find('.csv') is not -1 :        \n",
    "        items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(items)) :\n",
    "    \n",
    "    item = items[i]\n",
    "    df = pd.read_csv(item, engine = 'python').iloc[:,1:]\n",
    "    df.columns = [items[i]]\n",
    "    \n",
    "    if i == 0 :\n",
    "        corr_df = pd.DataFrame()\n",
    "        \n",
    "    corr_df = pd.concat([corr_df, df], axis = 1)\n",
    "\n",
    "    \n",
    "corr = np.array(corr_df.corr().mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = []\n",
    "\n",
    "for i in range(0, len(items)) :\n",
    "    score = items[i].split('=')[-1][:6]\n",
    "    score = score.split('.')[0]\n",
    "    score = float(score)\n",
    "    rmse.append(score)\n",
    "    \n",
    "df = pd.DataFrame({'rnk': list(range(0, len(rmse))), 'rmse': rmse, 'cor': corr})\n",
    "df.index = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('cor', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "g = sns.scatterplot(x=\"cor\", y=\"rmse\", data=df, s=30)\n",
    "\n",
    "for line in range(0, df.shape[0]):\n",
    "     g.text(df.cor[line]+0.00005 , df.rmse[line]-0.00003, \n",
    "            df.rnk[line], horizontalalignment='left', \n",
    "            size='medium', color='black', weight='semibold')\n",
    "        \n",
    "plt.xlim((df.cor.min()-0.00002,df.cor.max()+0.0002))\n",
    "plt.ylim((df.rmse.min()-0.0002,df.rmse.max()+0.0002))\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멱평균\n",
    "# 앙상블은 별도의 jupyter 파일에서 진행합니다.\n",
    "# 첫 번 째 조합\n",
    "# Ensemble_1 폴더를 만든 후 lgbm0=229.csv, rf4=231.csv를 위치시킨다.\n",
    "# 두 파일을 멱평균을 실시한다. / p값은 21.2로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Ensemble_1')\n",
    "# 경로를 위치시킨다.\n",
    "source_files = 'C:\\\\Users\\\\yena1\\\\Desktop\\\\3rd_Solution\\\\'\n",
    "destination_folder = 'C:\\\\Users\\\\yena1\\\\Desktop\\\\3rd_Solution\\\\Ensemble_1\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.move(source_files + 'lgbm0=229.csv', destination_folder + 'lgbm0=229.csv')\n",
    "shutil.move(source_files + 'rf4=231.csv', destination_folder + 'rf4=231.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용법: \n",
    "# 1) 스크립트를 실행하기 전에 Ensemble 폴더를 먼저 만듭니다. \n",
    "# 2) 앙상블할 submission 화일을 Ensemble 폴더에 저장합니다.\n",
    "# 3) 실행하면 현재 폴더에 앙상블한 submission 화일이 생성됩니다.\n",
    "\n",
    "# 주) 이 스크립트는 Kaggle Kernel에서 실행할 수 없고 여러분의 Jupyter Notebook에서 실행해야 합니다.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "folder = 'Ensemble_1'\n",
    "\n",
    "nf = 0\n",
    "for f in os.listdir(folder):\n",
    "    ext = os.path.splitext(f)[-1]\n",
    "    if ext == '.csv': \n",
    "        s = pd.read_csv(folder+\"/\"+f)\n",
    "    else: \n",
    "        continue\n",
    "    if len(s.columns) !=2:\n",
    "        continue\n",
    "    if nf == 0: \n",
    "        slist = s\n",
    "    else: \n",
    "        slist = pd.merge(slist, s, on=\"id\")\n",
    "    nf += 1\n",
    "\n",
    "# 이 파라미터는 멱평균 앙상블에 있어 중요한 수치임. 최적의 수치를 찾기 바랍니다.    \n",
    "p = 21\n",
    "\n",
    "if nf >= 2:\n",
    "    pred = 0\n",
    "    for j in range(nf): pred = pred + slist.iloc[:,j+1]**p \n",
    "    pred = pred / nf    \n",
    "    pred = pred**(1/p)\n",
    "\n",
    "    submit = pd.DataFrame({'id': slist.id, '18~20_ride': pred})\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = \"0+4.csv\"\n",
    "    submit.to_csv(fname, index=False)\n",
    "    \n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 조합\n",
    "# 사용법: \n",
    "# 1) 스크립트를 실행하기 전에 Ensemble 폴더를 먼저 만듭니다. \n",
    "# 2) 앙상블할 submission 화일을 Ensemble 폴더에 저장합니다.\n",
    "# 3) 실행하면 현재 폴더에 앙상블한 submission 화일이 생성됩니다.\n",
    "\n",
    "# 주) 이 스크립트는 Kaggle Kernel에서 실행할 수 없고 여러분의 Jupyter Notebook에서 실행해야 합니다.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "folder = 'Ensemble_2'\n",
    "\n",
    "nf = 0\n",
    "for f in os.listdir(folder):\n",
    "    ext = os.path.splitext(f)[-1]\n",
    "    if ext == '.csv': \n",
    "        s = pd.read_csv(folder+\"/\"+f)\n",
    "    else: \n",
    "        continue\n",
    "    if len(s.columns) !=2:\n",
    "        continue\n",
    "    if nf == 0: \n",
    "        slist = s\n",
    "    else: \n",
    "        slist = pd.merge(slist, s, on=\"id\")\n",
    "    nf += 1\n",
    "\n",
    "# 이 파라미터는 멱평균 앙상블에 있어 중요한 수치임. 최적의 수치를 찾기 바랍니다.    \n",
    "p = 21\n",
    "\n",
    "if nf >= 2:\n",
    "    pred = 0\n",
    "    for j in range(nf): pred = pred + slist.iloc[:,j+1]**p \n",
    "    pred = pred / nf    \n",
    "    pred = pred**(1/p)\n",
    "\n",
    "    submit = pd.DataFrame({'id': slist.id, '18~20_ride': pred})\n",
    "    t = pd.Timestamp.now()\n",
    "    fname = \"1+3.csv\"\n",
    "    submit.to_csv(fname, index=False)\n",
    "    \n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 산술 평균(가중치)\n",
    "first = pd.read_csv('0+4.csv')\n",
    "second = pd.read_csv('1+3.csv')\n",
    "third = pd.read_csv('rf2=238.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = '18~20_ride'\n",
    "\n",
    "w1, w2, w3 = 0.22, 0.30, 0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = w1*first[target] + w2*second[target] + w3*third[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 서브미션 파일 만들기\n",
    "\n",
    "sub = pd.read_csv('submission_sample.csv')\n",
    "sub[target] = W\n",
    "sub.to_csv('Final_submission.csv')\n",
    "\n",
    "print('finish .. !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
