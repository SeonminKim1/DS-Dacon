{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전력 수요량 예측 경진대회 코드리뷰 - 2위\n",
    "- https://dacon.io/competitions/official/196878/codeshare/416?page=1&dtype=recent&ptype=pub\n",
    "\n",
    "### 배경\n",
    "- 전력수요예측 시뮬레이션을 통한 효율적인 빅데이터 분석기법 발굴\n",
    "- 전력 융합신서비스 발굴 및 비즈니스 모델 개발 활용\n",
    "- 창의적인 아이디어와 빅데이터 분석기술을 토대로 에너지 신서비스 개발 촉진\n",
    "\n",
    "### 문제\n",
    "- 본 대회에서는, 기존 전력 사용 기록과 기상 데이터 등 공공 데이터를 이용하여, 각 가정 및 회사의 시간별, 일별, 월별 전력 사용량을 예측합니다. **2018년 7월1일부터 2018년 - 11월 30일까지의 에너지 사용량을 예측**합니다. 보다 정확히는 다음을 예측합니다.\n",
    "- 2018년 **7월 1일 00시부터 24시까지**, 24시간, ‘시간당 전력사용량’ (24개) \n",
    "- 2018년 **7월 1일부터 7월10일까지, 10일간**, ‘일간 전력사용량’ (10개)\n",
    "- 2018년 **7월 부터 11월까지, 5개월간**, ‘월간 전력사용량’ (5개)\n",
    "- 즉 각 세대(또는 상가)당 39개(24개,10개,5개)의 값을 예측해야 합니다.\n",
    "\n",
    "### 데이터\n",
    "- 본 대회에서는 2016년 7월 20일부터 2018년 6월 30일까지의 국내 특정 지역의 아파트들과 상가의 전력에너지 사용량이 주어집니다. 자세한 사항은 첨부파일로 포함합니다.\n",
    "- * 주의: 제공되는 데이터에는 결측치나 이상치(NA, 0인 값)가 포함되어 있습니다. 대회참가자들은 이러한 결측치를 고려하여, 결측치 처리 등 예측 기법을 적용하여야 합니다.\n",
    "- 참고로, NA가 발생한 경우, 직전 시간의 전력사용량 값이 상당히 큰 경향이 있습니다. 이는 미터링 데이터 수집 시스템의 특징으로 보입니다. 그러나 반드시 그런 것은 아닙니다.\n",
    "\n",
    "#### ① train.csv\n",
    "- 국내(인천지역) 특정 지역의 모 아파트 및 모 상가의 전력사용량. (1300호)\n",
    "- 2016년 7월 26일 11시 ~ 2018년 6월 30일 24시까지 시간 당 전력사용량\n",
    "\n",
    "#### ② test.csv\n",
    "- 국내(인천지역) 특정 지역의 모 아파트 및 모 상가의 전력사용량. (200호)\n",
    "- 2017년 7월 1일 00시 ~ 2018년 6월 30일 24시까지 시간 당 전력사용량\n",
    "* train의 세대와 다른 세대\n",
    "\n",
    "#### ③ submission.csv\n",
    "- test셋에 제시된 세대(상가)에 대한 예측값들을 제출하는 포맷\n",
    "- 본 대회에서는 2016년 7월 20일부터 2018년 6월 30일까지의 국내 특정 지역의 아파트들과 상가의 전력에너지 사용량이 주어집니다. 자세한 사항은 첨부파일로 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section [1]: Loading data............... Completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd             #데이터 전처리\n",
    "import numpy as np              #데이터 전처리\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#%% ============================== < Main > =======================================\n",
    "#%% Section #1: Loading data...    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# <test> : test.csv\n",
    "# <submission> : Data frame for submission.\n",
    "#------------------------------------------------------------------------------ \n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/submission.csv')\n",
    "\n",
    "# 중간중간 결과 출력해주는 함수\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# 원래 object형 -> Time 형으로 변환해줌\n",
    "test['Time'] = pd.to_datetime(test.Time)\n",
    "test = test.set_index('Time')\n",
    "\n",
    "print('Section [1]: Loading data............... Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NX1301</th>\n",
       "      <th>NX1302</th>\n",
       "      <th>NX1303</th>\n",
       "      <th>NX1304</th>\n",
       "      <th>NX1305</th>\n",
       "      <th>NX1306</th>\n",
       "      <th>NX1307</th>\n",
       "      <th>NX1308</th>\n",
       "      <th>NX1309</th>\n",
       "      <th>NX1310</th>\n",
       "      <th>...</th>\n",
       "      <th>NX1491</th>\n",
       "      <th>NX1492</th>\n",
       "      <th>NX1493</th>\n",
       "      <th>NX1494</th>\n",
       "      <th>NX1495</th>\n",
       "      <th>NX1496</th>\n",
       "      <th>NX1497</th>\n",
       "      <th>NX1498</th>\n",
       "      <th>NX1499</th>\n",
       "      <th>NX1500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-07-01 00:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01 01:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01 02:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01 03:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.020</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01 04:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.021</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     NX1301  NX1302  NX1303  NX1304  NX1305  NX1306  NX1307  \\\n",
       "Time                                                                          \n",
       "2017-07-01 00:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2017-07-01 01:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2017-07-01 02:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2017-07-01 03:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2017-07-01 04:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "                     NX1308  NX1309  NX1310  ...  NX1491  NX1492  NX1493  \\\n",
       "Time                                         ...                           \n",
       "2017-07-01 00:00:00     NaN     NaN     NaN  ...     NaN     NaN     NaN   \n",
       "2017-07-01 01:00:00     NaN     NaN     NaN  ...     NaN     NaN     NaN   \n",
       "2017-07-01 02:00:00     NaN     NaN     NaN  ...     NaN     NaN     NaN   \n",
       "2017-07-01 03:00:00     NaN     NaN     NaN  ...     NaN     NaN     NaN   \n",
       "2017-07-01 04:00:00     NaN     NaN     NaN  ...     NaN     NaN     NaN   \n",
       "\n",
       "                     NX1494  NX1495  NX1496  NX1497  NX1498  NX1499  NX1500  \n",
       "Time                                                                         \n",
       "2017-07-01 00:00:00     NaN     NaN     NaN     NaN   0.275   0.021     NaN  \n",
       "2017-07-01 01:00:00     NaN     NaN     NaN     NaN   0.222   0.021     NaN  \n",
       "2017-07-01 02:00:00     NaN     NaN     NaN     NaN   0.237   0.021     NaN  \n",
       "2017-07-01 03:00:00     NaN     NaN     NaN     NaN   0.229   0.020     NaN  \n",
       "2017-07-01 04:00:00     NaN     NaN     NaN     NaN   0.202   0.021     NaN  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 40)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meter_id</th>\n",
       "      <th>X2018_7_1_1h</th>\n",
       "      <th>X2018_7_1_2h</th>\n",
       "      <th>X2018_7_1_3h</th>\n",
       "      <th>X2018_7_1_4h</th>\n",
       "      <th>X2018_7_1_5h</th>\n",
       "      <th>X2018_7_1_6h</th>\n",
       "      <th>X2018_7_1_7h</th>\n",
       "      <th>X2018_7_1_8h</th>\n",
       "      <th>X2018_7_1_9h</th>\n",
       "      <th>...</th>\n",
       "      <th>X2018_7_6_d</th>\n",
       "      <th>X2018_7_7_d</th>\n",
       "      <th>X2018_7_8_d</th>\n",
       "      <th>X2018_7_9_d</th>\n",
       "      <th>X2018_7_10_d</th>\n",
       "      <th>X2018_7_m</th>\n",
       "      <th>X2018_8_m</th>\n",
       "      <th>X2018_9_m</th>\n",
       "      <th>X2018_10_m</th>\n",
       "      <th>X2018_11_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NX1301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NX1302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NX1303</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NX1304</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NX1305</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  meter_id  X2018_7_1_1h  X2018_7_1_2h  X2018_7_1_3h  X2018_7_1_4h  \\\n",
       "0   NX1301             0             0             0             0   \n",
       "1   NX1302             0             0             0             0   \n",
       "2   NX1303             0             0             0             0   \n",
       "3   NX1304             0             0             0             0   \n",
       "4   NX1305             0             0             0             0   \n",
       "\n",
       "   X2018_7_1_5h  X2018_7_1_6h  X2018_7_1_7h  X2018_7_1_8h  X2018_7_1_9h  ...  \\\n",
       "0             0             0             0             0             0  ...   \n",
       "1             0             0             0             0             0  ...   \n",
       "2             0             0             0             0             0  ...   \n",
       "3             0             0             0             0             0  ...   \n",
       "4             0             0             0             0             0  ...   \n",
       "\n",
       "   X2018_7_6_d  X2018_7_7_d  X2018_7_8_d  X2018_7_9_d  X2018_7_10_d  \\\n",
       "0            0            0            0            0             0   \n",
       "1            0            0            0            0             0   \n",
       "2            0            0            0            0             0   \n",
       "3            0            0            0            0             0   \n",
       "4            0            0            0            0             0   \n",
       "\n",
       "   X2018_7_m  X2018_8_m  X2018_9_m  X2018_10_m  X2018_11_m  \n",
       "0          0          0          0           0           0  \n",
       "1          0          0          0           0           0  \n",
       "2          0          0          0           0           0  \n",
       "3          0          0          0           0           0  \n",
       "4          0          0          0           0           0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================ <Coding 구성> =======================================\n",
    "# [Local Function #1]: SAMPE calculation module\n",
    "# [Local Function #2]: AR_data_set calculation module = 시계열 데이터 추출 모듈\n",
    "# [Local Function #3]: AR_day_set calculation module = 시계열 day set 추출 모듈\n",
    "# [Local Function #4]: linear_prediction calculation module = 선형 계산 모듈\n",
    "# [Local Function #5]: Random forest module = 랜덤 포레스트 모듈\n",
    "# [Local Function #6]: DNN module = DNN 모듈\n",
    "\n",
    "# [Main function]\n",
    "# [Section #1]: Data loading section = 데이터 로딩\n",
    "# [Section #2]: Data generation for training set = train 데이터 셋 생성\n",
    "# [Section #3]: Anormaly detection using AR model  = 이상치 탐지\n",
    "# [Section #4]: data prediction for hour profile = 시간단위의 데이터 예측\n",
    "# [Section #5]: data prediction for day profile = 일단위의 데이터 예측\n",
    "# [Section #6]: data prediction for month profile = 월단위의 데이터 예측\n",
    "#============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Local function #1 - <SAMPE calculation module>\n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# <A> : Real data.\n",
    "# <F> : Forecasting data.\n",
    "#------------------------------------------------------------------------------ \n",
    "# [Output] # 평가지표 SMAPE로 값 구하기\n",
    "def smape(A, F): # A는 Real Data, F는 Forecaseting Data\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Input]\n",
    "# < Data > : test.csv.\n",
    "# < place_id > : power meter ID.\n",
    "# < prev_type > : 예측 전 날의 데이터 타입(<1> : Workday <2> : Weekend)\n",
    "# < Curr_type >: 예측 날의 데이터 타입(<1> : Workday <2> : Weekend)  \n",
    "#------------------------------------------------------------------------------ \n",
    "# [Output]\n",
    "# < TrainAR >: 예측 전날의 data set [day x time]\n",
    "# < TestAR >: 예측 날의 data set [day x time]\n",
    "#------------------------------------------------------------------------------ \n",
    "# %% Local function <AR_data_set calculation module>\n",
    "# 시간 데이터 예측을 위한 데이터 셋 추출 함수.\n",
    "# test.csv 파일 내에 전력 데이터를 요일 타입을 고려하여 분류함.\n",
    "# 전날 데이터를 학습하여 다음날을 예측하는 방식을 사용하며, 요일 타입은 2가지로 \n",
    "# 분류함. 월~금은 Workday, 토~일은 weekend    \n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# test.csv, meter ID, 예측 전날의 데이터 타입, 예측 날의 데이터 타입 \n",
    "def AR_data_set(Data, place_id, prev_type, Curr_type):\n",
    "    # Mon: 0 ~ Sun:6\n",
    "    TrainAR = []; TestAR = []\n",
    "    len_bad = 20  # 하루 내 NaN의 개수 기준, (24-len_bad)보다 많으면 그 날은 제거 \n",
    "    Power = Data[place_id].iloc  # test.csv에서 특정 id의 전력 데이터 - NX1301, NX1302, NX1303 컬럼들의 행들 이런식으로..\n",
    "    Date = Data[place_id].index  # test.csv에서 특정 id의 날짜 데이터 - 뽑은 컬럼에 대한 인덱스 (Date Time 으로 변환했으니 Date)\n",
    "        \n",
    "    # 예측전날, 예측날들의 데이터들에 대한 다 일단 [0]으로\n",
    "    prev_aloc = [0]*24;  curr_aloc = [0]*24  # pre-allocation\n",
    "    \n",
    "    # 모든 행 점검 - bad data  - 7104 행부터는 제대로 나옴.\n",
    "    for ii in range(24,len(Date)): # ii 는 24 ~ 전체 행의 갯수\n",
    "        # Date - datetime 형태의 index => 시간이 자정이거나, 48행이후부터거나, \n",
    "        # 과거 1시꺼 데이터 * 24가 prev_aloc 다 합친 것과 같지 않을떄?\n",
    "        # 현재 1시꺼 데이터 * 24가 curr_aloc 다 합친것과 같지 않을 때?\n",
    "        \n",
    "        # 일주일마다 이 if 가 출력됨.\n",
    "        if (Date[ii].hour == 0) & (ii > 48) & (np.sum(curr_aloc) != 24 * curr_aloc[1]) & (np.sum(prev_aloc) != 24 * prev_aloc[1]): \n",
    "            prev_idx = 0;  curr_idx = 0      # bad data idx\n",
    "            # print(ii, '. prev_aloc, curr_aloc', prev_aloc, curr_aloc) # ii가 168씩 증가한다? => 1주일마다\n",
    "            \n",
    "################## 예측 전날에 대한 보간법 - 하루씩 봐야됨\n",
    "            for kk in range(0,24): # kk 는 하루 24시간\n",
    "                if prev_aloc[kk]>-20: # check the bad data.\n",
    "                    prev_idx = prev_idx+1     \n",
    "                    #print('bad_data', Data[ii])\n",
    "                    \n",
    "                # bad data가 아닐 때\n",
    "                else: # interpolate the bad data.\n",
    "                    # bad data일 경우, 앞뒤로 20개의 포인트를 가져와서 \n",
    "                    # interpolation 진행. - 보간법\n",
    "                    temp = np.zeros([1,41]) \n",
    "                    for qq in range(0,41): # Power NX1301 의 행 전체\n",
    "                        temp[0,qq] = Power[(ii-24)-(24-kk)-20+qq]\n",
    "                    \n",
    "                    # df 로 변환\n",
    "                    temp_temp = pd.DataFrame(data = temp)\n",
    "\n",
    "                    # data frame 형으로 바꿔서 보간법만 진행하고 다시 값들만 집어넣음.\n",
    "                    temp = temp_temp.interpolate('spline',order =1) # 보간법 진행\n",
    "                    temp = temp.values # 값들만\n",
    "                    prev_aloc[kk] = temp[0,20]\n",
    "                    \n",
    "######################### 예측날에 대한 보간법\n",
    "            for kk in range(0,24): # 하루 24시간?\n",
    "                if curr_aloc[kk]>-20:       # check the bad data.\n",
    "                    curr_idx =curr_idx+1\n",
    "                else:\n",
    "                    # bad data일 경우, 앞뒤로 20개의 포인트를 가져와서 \n",
    "                    # interpolation 진행.\n",
    "                    temp = np.zeros([1,41])\n",
    "                    for qq in range(0,41):\n",
    "                        temp[0, qq] = Power[(ii)-(24-kk)-20+qq] # ii 는 30, kk는 12, qq는 -25 ~ 15\n",
    "                        # temp[0, -25] ~ temp[0,15]\n",
    "                    \n",
    "                    # df 로 변환\n",
    "                    temp_temp = pd.DataFrame(data = temp)\n",
    "                    \n",
    "                    # data frame 형으로 바꿔서 보간법만 진행하고 다시 값들만 집어넣음.\n",
    "                    temp = temp_temp.interpolate('spline',order =1)\n",
    "                    temp = temp.values\n",
    "                    curr_aloc[kk] = temp[0,20]\n",
    "\n",
    "######################### \n",
    "            # bad data가 특정 개수 이상이면, data set에 추가하지 않는다.  \n",
    "            # TrainAR, TestAR 다시 구성한 것.\n",
    "            if (prev_idx>len_bad)&(curr_idx>len_bad):\n",
    "                TrainAR.append(prev_aloc)\n",
    "                TestAR.append(curr_aloc)\n",
    "                        \n",
    "        # 0시에 하루 데이터 초기화.                     \n",
    "        if Date[ii].hour == 0:\n",
    "            prev_aloc = [0]*24\n",
    "            curr_aloc = [0]*24\n",
    "        \n",
    "        # 요일 데이터 확인.\n",
    "        prev_day = Date[ii-24].weekday() # 하루전날의 요일\n",
    "        curr_day = Date[ii].weekday() # 현재날의 요일\n",
    "        \n",
    "        # 요일 데이터 타입 분류\n",
    "        # Workday(1) = day type<5(월~금) - 평일\n",
    "        # Workday(2) = day type>=5(토~일) - 주말\n",
    "        \n",
    "########### 하루전날에 대한 전력데이터, 현재에 대한 전력데이터 prev_aloc, curr_aloc로 구함.\n",
    "        # 전날이 주말아니고 요일(월-금)이고, 현재날짜가 주말이고, 요일(토-일) 일 때 ==> 평일-주말\n",
    "        if ((prev_type ==1) & (prev_day<5)) & ((Curr_type ==2) & (curr_day>4)): # <1> : Workday <2> : Weekend        \n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24] # 하루전날에 대한 전력데이터\n",
    "            curr_aloc[Date[ii].hour] = Power[ii] # 현재에 대한 전력데이터\n",
    "        \n",
    "        # 전날이 주말아니고 요일(월-금)이고, 현재날짜가 평일이고, 요일(월-금) 일 때 == > 평일-평일\n",
    "        if ((prev_type ==1)&(prev_day<5))&((Curr_type ==1)&(curr_day<5)): # <1> : Workday <2> : Weekend\n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24]\n",
    "            curr_aloc[Date[ii].hour] = Power[ii]\n",
    "            \n",
    "        # 전날이 주말이고 요일(토-일)이고, 현재날짜가 주말이고, 요일(토-일) 일 때 ==> 주말-주말\n",
    "        if ((prev_type ==2)&(prev_day>4))&((Curr_type ==2)&(curr_day>4)): # <1> : Workday <2> : Weekend\n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24]\n",
    "            curr_aloc[Date[ii].hour] = Power[ii]\n",
    "        \n",
    "        # 전날이 주말이고 요일(토-일이고, 현재날짜가) 평일이고, 요일 (월-금)일 때 ==> 주말-평일\n",
    "        if ((prev_type ==2)&(prev_day>4))&((Curr_type ==1)&(curr_day<5)):           \n",
    "            prev_aloc[Date[ii-24].hour] = Power[ii-24]\n",
    "            curr_aloc[Date[ii].hour] = Power[ii]\n",
    "\n",
    "    TrainAR = np.array(TrainAR)\n",
    "    TestAR = np.array(TestAR)\n",
    "    return TrainAR, TestAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Local function < AR_day_set calculation module >\n",
    "# 하루 사용 데이터 예측을 위한 데이터 셋 추출 함수.\n",
    "# test.csv 파일 내에 전력 데이터를 요일 타입을 고려하여 분류함.\n",
    "\n",
    "# <요일 타입>: 월 ~ 일\n",
    "# Similar day approach method만을 활용할 예정이기 때문에 최근 데이터 6주만을 \n",
    "# 정리함.    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# <Data> : test.csv.\n",
    "# <place_id>: power meter ID.\n",
    "#------------------------------------------------------------------------------ \n",
    "# [Output]\n",
    "# <temp_day>: 과거 하루 전력 사용량  [week number(최근 순) x day type]\n",
    "#------------------------------------------------------------------------------     \n",
    "def AR_day_set(data, place_id):\n",
    "    Power = data[place_id].values       #전력 데이터 \n",
    "    Date = data[place_id].index         #요일 데이터 \n",
    "    \n",
    "    temp_day = np.zeros([6, 7])         # 6행 7열 짜리 pre-allocation for output dataset\n",
    "    mon_idx = np.zeros([1, 7])          # 1행 7열 짜리 몇 번째 week인지 확인하는 idx\n",
    "    \n",
    "    # 6주로 보는건 => 월단위가 길어봤자 6주\n",
    "    # 행에서 500을 왜 뻄?  \n",
    "    for ii in range(0,len(Power)-500):\n",
    "        idx = len(Power) - ii -1 # index \n",
    "        day_idx = Date[idx].weekday()       # data의 요일정보\n",
    "        time_idx = Date[idx].hour           # data의 시간정보 \n",
    "        \n",
    "        if mon_idx[0, day_idx] < 6:         # 6번째 week 이상이면 추가 X\n",
    "            \n",
    "            # nan 값 처리. 1주전, 2주전, 3주전 것을 사용.\n",
    "            if np.isnan(Power[idx]):        # bad data restortion\n",
    "                res_data = np.zeros([1, 9]) # 1행 9열\n",
    "                \n",
    "                # 1주전, 2주전, 3주전의 같은 요일, 시간 데이터를 저장 후 mean\n",
    "                res_data[0,0] = Power[idx-24*7-1]\n",
    "                res_data[0,1] = Power[idx-24*7]\n",
    "                res_data[0,2] = Power[idx-24*7+1]\n",
    "                \n",
    "                res_data[0,3] = Power[idx-48*7-1]\n",
    "                res_data[0,4] = Power[idx-48*7]\n",
    "                res_data[0,5] = Power[idx-48*7+1]\n",
    "                \n",
    "                res_data[0,6] = Power[idx-1]\n",
    "                res_data[0,7] = Power[idx-3*24*7]\n",
    "                res_data[0,8] = Power[idx+1]\n",
    "                \n",
    "                # 하루 사용량 저장을 위한 시간 데이터 합 - nanmean = nan 값 무시하고 mean 집어넣음.\n",
    "                # temp_day의 해당 주차의 \n",
    "                temp_day[int(round(mon_idx[0,day_idx])), day_idx] = temp_day[int(round(mon_idx[0,day_idx])), day_idx]+ np.nanmean(res_data)\n",
    "    \n",
    "            # nan 값 보정 처리 필요 없을 때\n",
    "            else:\n",
    "                # 하루 사용량 저장을 위한 시간 데이터 합 - 전력값 합.\n",
    "                temp_day[int(round(mon_idx[0,day_idx])), day_idx] = temp_day[int(round(mon_idx[0,day_idx])), day_idx] + Power[idx]\n",
    "    \n",
    "            # \n",
    "            if time_idx == 0:\n",
    "                # 요일이 지나면, week확인 idx +1\n",
    "                mon_idx[0,day_idx] = mon_idx[0,day_idx] + 1\n",
    "    return temp_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function <linear_prediction calculation module>\n",
    "### 자기회귀모델인듯? !\n",
    "# 선형 예측 방식 구현. (Autoregressive model) - 자기회귀모델\n",
    "# Y(예측 날) = A(coefficient) * X(예측전날)\n",
    "# X-1(역행렬)*Y = A\n",
    "# A를 추출하여, 예측에 활용함.     \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# < trainAR >: test.csv의 예측 전날 데이터 셋 \n",
    "# < testAR >: test.csv의 예측 날 데이터 셋 \n",
    "# < flen >: filter length(예측전날의 몇개의 데이터를 가져다 쓸지 결정)\n",
    "# < test_data > : 실제로 예측 하고 싶은 전날의 데이터(TEST 데이터)    \n",
    "#------------------------------------------------------------------------------ \n",
    "# [Output]\n",
    "# < avr_smape > : Training set으로 테스트 했을 때, smape\n",
    "# < fcst > : test_data의 예측 결과 \n",
    "# < pred > : Traing set으로 예측했던 예측 결과    \n",
    "#------------------------------------------------------------------------------   \n",
    "# 예측전날 데이터셋 / 예측 날 데이터셋 / 예측전날에 몇개의 데이터를 가져다 쓸지 / 실제로 예측하고 싶은 TEST 데이터\n",
    "\n",
    "# 자기회귀모델을 돌려서 \n",
    "# 예측 날 전기 사용량 (Y) = 계수 * 예측 전날 전기 사용량\n",
    "def linear_prediction(trainAR, testAR, flen, test_data):\n",
    "    len_tr = len(trainAR[0,:])   # 시간 포인트 수 \n",
    "    day_t = len(trainAR)\n",
    "    pred = np.empty((len(trainAR),len_tr))\n",
    "    fcst = np.empty((len(trainAR),len_tr))\n",
    "    \n",
    "    for j in range(0, day_t):\n",
    "        if day_t>1:\n",
    "            x_ar=np.delete(trainAR[:,len_tr-flen:len_tr], (j), axis=0)\n",
    "            y=np.delete(testAR, (j), axis=0)\n",
    "        else:\n",
    "            x_ar = trainAR[:,len_tr-flen:len_tr]\n",
    "            y = testAR\n",
    "            \n",
    "        pi_x_ar = np.linalg.pinv(x_ar)\n",
    "        lpc_c = np.empty((len(x_ar),flen))\n",
    "        lpc_c=np.matmul(pi_x_ar, y)\n",
    "        \n",
    "        test_e = trainAR[j,:]\n",
    "        test_ex = test_e[len_tr-flen:len_tr]\n",
    "        pred[j,:]=np.matmul(test_ex, lpc_c)  \n",
    "        \n",
    "    x_ar = trainAR[:,len_tr-flen:len_tr]\n",
    "    y = testAR\n",
    "    pi_x_ar = np.linalg.pinv(x_ar)\n",
    "    lpc_c = np.empty((len(x_ar),flen))\n",
    "    lpc_c=np.matmul(pi_x_ar, y)\n",
    "    Test_AR = testAR[0:len(testAR),:]\n",
    "    smape_list=np.zeros((len(pred),1))\n",
    "\n",
    "    for i in range(0,len(pred)):\n",
    "        smape_list[i]=smape(pred[i,:], Test_AR[i,:])\n",
    "        avr_smape = np.mean(smape_list)  \n",
    "    \n",
    "    test_e = test_data\n",
    "    test_ex = test_e[len_tr-flen:len_tr]   \n",
    "    fcst = np.matmul(test_ex,lpc_c)\n",
    "    \n",
    "    return avr_smape, fcst, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function < Similar day approach module >\n",
    "# Similar day approaach method 구현.\n",
    "# 같은 요일 타입의 날의 데이터를 N개를 추출하여 평균을 취하여 사용함.    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# < trainAR >: test.csv의 예측 전날 데이터 셋 \n",
    "# < testAR > : test.csv의 예측 날 데이터 셋 \n",
    "# < slen >: 추출한 날의 수 (N)\n",
    "# < sim_set >: 실제로 예측 하고 싶은 전날의 데이터(TEST 데이터)    \n",
    "#------------------------------------------------------------------------------ \n",
    "# [Output]\n",
    "# < simil_smape >: Training set으로 테스트 했을 때, smape\n",
    "# < simil_temp > : test_data의 예측 결과   \n",
    "#------------------------------------------------------------------------------\n",
    "def similar_approach(trainAR, testAR, slen, sim_set):\n",
    "    simil_smape_list = np.zeros([1,len(testAR[:,0])])\n",
    "    \n",
    "    for col_ii in range(0,len(testAR[:,0])):    \n",
    "        simil_mean = []\n",
    "        simil_temp =np.zeros([1,24])\n",
    "        simil_idx =np.zeros([1,len(testAR[:,0])])\n",
    "        \n",
    "        for sub_col in range(0,len(testAR[:,0])):\n",
    "            simil_idx[0,sub_col] = smape(trainAR[col_ii,:],trainAR[sub_col,:])\n",
    "            \n",
    "        testAR_temp = np.delete(testAR, np.argmin(simil_idx), axis=0)\n",
    "        simil_idx = np.delete(simil_idx, np.argmin(simil_idx), axis=1)\n",
    "        \n",
    "        for search_len in range(0,slen):\n",
    "            simil_mean.append(testAR_temp[np.argmin(simil_idx), :])\n",
    "            testAR_temp = np.delete(testAR, np.argmin(simil_idx), axis=0)\n",
    "            simil_idx = np.delete(simil_idx, np.argmin(simil_idx), axis=1)\n",
    "              \n",
    "        for row_ii in range(0, 24):           \n",
    "            simil_temp[0, row_ii] = np.median(testAR_temp[:,row_ii])\n",
    "\n",
    "        simil_smape_list[0,col_ii] = smape(testAR[col_ii,:], simil_temp)\n",
    "        simil_smape  = np.mean(simil_smape_list)\n",
    "    \n",
    "    simil_mean = []\n",
    "    simil_temp =np.zeros([1,24])\n",
    "    simil_idx =np.zeros([1,len(testAR[:,0])])\n",
    "    testAR_temp = testAR\n",
    "    \n",
    "    for sub_col in range(0,len(testAR[:,0])):\n",
    "        simil_idx[0,sub_col] = smape(sim_set,trainAR[sub_col,:])\n",
    "\n",
    "    for search_len in range(0,slen):\n",
    "        simil_mean.append(testAR_temp[np.argmin(simil_idx),:])\n",
    "        testAR_temp = np.delete(testAR, np.argmin(simil_idx), axis=0)\n",
    "        simil_idx = np.delete(simil_idx, np.argmin(simil_idx), axis=1)\n",
    "   \n",
    "    for row_ii in range(0, 24):           \n",
    "        simil_temp[0, row_ii] = np.median(testAR_temp[:,row_ii])\n",
    "    \n",
    "    return simil_smape, simil_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Local function < Random forest module > \n",
    "# Random forest를 이용한 regression 기반의 forecasting algorithm\n",
    "# 전날의 24시간 프로파일을 이용해 다음날 24시간의 프로파일을 예측하는 시스템.   \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# < trainAR > : test.csv의 예측 전날 데이터 셋 \n",
    "# < testAR > : test.csv의 예측 날 데이터 셋 \n",
    "# < x_24hrs > : 실제로 예측 하고 싶은 전날의 데이터(TEST 데이터)  \n",
    "#------------------------------------------------------------------------------ \n",
    "# [Output]\n",
    "# < ypr > : x_24hrs를 이용한 예측 결과  \n",
    "# < avr_smape > : validation set으로 확인한, smape      \n",
    "#------------------------------------------------------------------------------\n",
    "def machine_learn_gen(trainAR, testAR, x_24hrs):\n",
    "    Dnum = trainAR.shape[0]; lnum = trainAR.shape[1]\n",
    "    smape_list = np.zeros([Dnum,1])\n",
    "    \n",
    "    for ii in range(0, Dnum): # cross validation\n",
    "        trainAR_temp = np.delete(trainAR, ii, axis=0)\n",
    "        testAR_temp  = np.delete(testAR, ii, axis=0)\n",
    "        \n",
    "        # mae 기반의 loss를 이용한 randomforest model 생성\n",
    "        regr = RandomForestRegressor(max_depth=2, random_state=0,n_estimators=100, criterion='mae')\n",
    "        regr.fit(trainAR_temp, testAR_temp)\n",
    "\n",
    "        x_temp = np.zeros([1,lnum])\n",
    "        for kk in range(0,lnum):\n",
    "            x_temp[0,kk] = trainAR[ii, kk]\n",
    "            \n",
    "        ypr = regr.predict(x_temp)\n",
    "\n",
    "        yre = np.zeros([1,lnum])\n",
    "        for kk in range(0,lnum):\n",
    "            yre[0,kk] = testAR[ii, kk]\n",
    "        \n",
    "        smape_list[ii] = smape(np.transpose(ypr),np.transpose(yre))\n",
    "        \n",
    "    regr = RandomForestRegressor(max_depth=2, random_state=0,n_estimators=100, criterion='mae')\n",
    "    regr.fit(trainAR, testAR)\n",
    "        \n",
    "    x_24hrs = np.reshape(x_24hrs,(-1,lnum))\n",
    "    \n",
    "    avr_smape = np.mean(smape_list)\n",
    "    ypr=regr.predict(x_24hrs)\n",
    "    \n",
    "    return ypr,  avr_smape, smape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Local function < DNN module >\n",
    "# DNN를 이용한 regression 기반의 forecasting algorithm\n",
    "# 전날의 24시간 프로파일을 이용해 \n",
    "# 다음날 24시간의 프로파일을 예측할 수 있는 DNN 모델 생성.    \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# < trainAR >: test.csv의 예측 전날 데이터 셋 \n",
    "# < testAR > : test.csv의 예측 날 데이터 셋 \n",
    "# < EPOCHS > : DNN의 학습을 위한 epoch size   \n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# < model > : 학습된 모델 출력  \n",
    "# < avr_smape > : test set으로 테스트 했을 때의 평균, smape  \n",
    "# < smape_list >: test set으로 테스트 했을 때의 각각, smape      \n",
    "#------------------------------------------------------------------------------     \n",
    "def non_linear_model_gen(trainAR, testAR, EPOCHS):\n",
    "    numData=np.size(trainAR,0)\n",
    "    numTr=int(numData*0.8)\n",
    "    Xtr=trainAR[0:numTr-1,:]\n",
    "    Ytr=testAR[0:numTr-1,:]\n",
    "    \n",
    "    Xte=trainAR[numTr:numData,:]\n",
    "    Yte=testAR[numTr:numData,:]\n",
    "    \n",
    "    num_tr = np.size(trainAR,1)\n",
    "    num_te = np.size(testAR,1)\n",
    "    \n",
    "    def build_model():        \n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(num_tr,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(num_te)\n",
    "        ])\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "        \n",
    "        model.compile(loss='mae',\n",
    "                        optimizer=optimizer,\n",
    "                        metrics=['mae', 'mse'])\n",
    "        return model\n",
    "\n",
    "    model = build_model()\n",
    "    # model.summary()\n",
    "    \n",
    "    #example_batch = Xtr[:10]\n",
    "    #example_result = model.predict(example_batch)\n",
    "    #example_result\n",
    "    \n",
    "    class PrintDot(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            if epoch % 100 == 0: print('')\n",
    "\n",
    "    history = model.fit(\n",
    "      Xtr, Ytr,\n",
    "      epochs=EPOCHS, verbose=0,\n",
    "      callbacks=[PrintDot()])\n",
    "   \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    # hist.tail()\n",
    "    \n",
    "    Ypr = model.predict(Xte)\n",
    "    \n",
    "    smape_list=np.zeros((len(Ypr),1))\n",
    "    \n",
    "    for i in range(0,len(Ypr)):\n",
    "        smape_list[i]=smape(Ypr[i,:], Yte[i,:])\n",
    "    avr_smape=np.mean(smape_list)\n",
    "    \n",
    "    return model, avr_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ============================== < Main > =======================================\n",
    "#%% Section #2: Data generation for training set  \n",
    "#------------------------------------------------------------------------------\n",
    "# [Input]\n",
    "# < test > : Test set\n",
    "# < submission > : Data frame for submission.\n",
    "# < prev_type > : Day type of the previous day (workday = 1, weekend =2). # 이전날의 요일 type\n",
    "# < curr_type > : Day type of the current day (workday = 1, weekend =2). # 현재날의 요일 type\n",
    "#------------------------------------------------------------------------------ \n",
    "# [Output]\n",
    "# < testAR > : Test set\n",
    "# < trainAR > : Training set\n",
    "# < subm_24hrs > test data for 24hrs prediction\n",
    "#------------------------------------------------------------------------------ \n",
    "agg = {}; comp_smape =[]; key_idx = 0\n",
    "for key in test.columns:\n",
    "    key_idx = key_idx + 1\n",
    "    print('key, key_idx', [key, key_idx])\n",
    "    prev_type = 2   # 전날 요일 타입\n",
    "    curr_type = 2   # 예측날 요일 타입\n",
    "    \n",
    "    # shape -> trainAR(10,24) / testAR(10,24)\n",
    "    trainAR, testAR = AR_data_set(test, key, prev_type, curr_type) \n",
    "    \n",
    "    print('Section [2]: Data generation for training set...............')\n",
    "    \n",
    "    ### [시간 예측을 위한 마지막 24pnt 추출]\n",
    "    # NaN 값처리를 위해서 마지막 40pnts 추출 한 후에 \n",
    "    temp_test = test[key].iloc[8759-40:] # 끝에서 40개를 왜 보간법 다시?\n",
    "    temp_test = temp_test.interpolate(method='spline', order=2) # 보간법 - nan값 있을떄만 다시 채워주는듯? \n",
    "\n",
    "    # interpolation하고 나서 24pnts 재추출 \n",
    "    temp_test = np.array(temp_test.values)\n",
    "    temp_test = temp_test[len(temp_test)-24:len(temp_test)+1] # 40개 중에 24개 재추출\n",
    "    subm_24hrs = temp_test\n",
    "    del temp_test\n",
    "    \n",
    "# %% Section #3: Anormaly detection using AR model  = 이상치 탐지\n",
    "#------------------------------------------------------------------------------\n",
    "# [Input] : # < testAR > : Test set   # < trainAR >: Training set\n",
    "#------------------------------------------------------------------------------\n",
    "# [Output] : # < testAR > : Test set   # < trainAR >: Training set\n",
    "#------------------------------------------------------------------------------   \n",
    "    fchk = 1        # filter length\n",
    "    temp_idx = []; smape_lin = []\n",
    "    \n",
    "################## 이상치 탐지중 - Nan 값 제거\n",
    "    # 한 행씩 linear prediction을 테스트해보고 NaN이 발견된다면, 그 행을 제거.\n",
    "    for chk_bad in range(0, len(trainAR[:,0])):\n",
    "        prev_smape = 200 # SMAPE 기준값 \n",
    "        nan_chk = 0      # NaN chk idx\n",
    "        trainAR_temp = np.zeros([1,24])     # pre-allocation\n",
    "        testAR_temp = np.zeros([1,24])      # pre-allocation\n",
    "\n",
    "        # 한 행씩 테스트를 하기 위한 변수 설정\n",
    "        for ii in range(0,24):\n",
    "            trainAR_temp[0,ii] = trainAR[chk_bad,ii] # 한행씩 체크중\n",
    "            testAR_temp[0,ii] = testAR[chk_bad,ii] # 한행씩 체크중\n",
    "        \n",
    "        # linear prediction test - lin_sampe = () / fcst_temp = (24, ) / pred_hyb = (1, 24)\n",
    "        # samp 값, 미래값, 예측값\n",
    "        lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR_temp, testAR_temp, fchk, subm_24hrs)\n",
    "            \n",
    "        if np.isnan(lin_sampe):     # SMAPE가 NaN 경우, 그 행을 제거\n",
    "            nan_chk = 1\n",
    "        if np.isnan(np.sum(trainAR_temp)): # chk_bad의 행이 NaN을 포함할 경우 제거\n",
    "            nan_chk = 1         \n",
    "        if np.isnan(np.sum(testAR_temp)): # chk_bad의 행이 NaN을 포함할 경우 제거\n",
    "            nan_chk = 1\n",
    "        if nan_chk == 1: #NaN 값이 있는 행 넘버를 append\n",
    "            temp_idx.append(chk_bad)\n",
    "\n",
    "    # NaN 값이 나타난 data set은 제거 \n",
    "    trainAR = np.delete(trainAR, temp_idx, axis=0)\n",
    "    testAR = np.delete(testAR, temp_idx, axis=0)                 \n",
    "    # print('NaN 값이 나타난 행 (제거)', temp_idx) - > 다양한 항이 제거됨\n",
    "\n",
    "########################## filter length 최적화\n",
    "    del_smape = np.zeros([1,len(trainAR[:,1])])\n",
    "    prev_smape = 200\n",
    "    fchk = 0\n",
    "    \n",
    "    # filter length 최적화 \n",
    "    for chk in range(3,24):\n",
    "        # filter length을 바꿔가며 Smape가 최소가 되는 값을 찾아감.\n",
    "        lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR, testAR, chk, subm_24hrs)\n",
    "        if prev_smape>lin_sampe:\n",
    "            fchk = chk\n",
    "            prev_smape = lin_sampe    \n",
    "   \n",
    "    # 필요없는 데이터 제거\n",
    "    # 한 줄(하루)씩 제거해가면서 SMAPE 결과를 분석.\n",
    "    for chk_lin in range(0,len(trainAR[:,1])):\n",
    "        trainAR_temp = np.delete(trainAR, chk_lin, axis=0)\n",
    "        testAR_temp = np.delete(testAR, chk_lin, axis=0)\n",
    "        lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR_temp, testAR_temp, fchk, subm_24hrs)          \n",
    "         \n",
    "        del_smape[0,chk_lin] = lin_sampe\n",
    "    \n",
    "    # SMAPE에 악영향을 주는 행을 제거      \n",
    "    trainAR = np.delete(trainAR, np.argmin(del_smape), axis=0)\n",
    "    testAR = np.delete(testAR, np.argmin(del_smape), axis=0)\n",
    "    del_smape = np.delete(del_smape, np.argmin(del_smape), axis =1)\n",
    "    print('Section [3]: mitigating bad data...............')\n",
    "    del nan_chk, lin_sampe, fcst_temp, pred_hyb, prev_smape, temp_idx\n",
    "    \n",
    "#%% Section #4: Prediction test\n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# <fcst> : Predicted hour profile result \n",
    "#------------------------------------------------------------------------------ \n",
    "    # DNN model \n",
    "    EPOCHS = 1 # 80\n",
    "    Non_NNmodel, non_smape = non_linear_model_gen(trainAR, testAR, EPOCHS)\n",
    "    \n",
    "    # random forest model\n",
    "    mac_fcst, Mac_smape, smape_listss = machine_learn_gen(trainAR, testAR,subm_24hrs)\n",
    "    \n",
    "    # linear model\n",
    "    lin_sampe, fcst_temp, pred_hyb = linear_prediction(trainAR, testAR, fchk, subm_24hrs)\n",
    "    \n",
    "    # Similar day approach model\n",
    "    temp_24hrs = np.zeros([1,24])    # np.array type으로 변경. \n",
    "    for qq in range(0,24):\n",
    "        temp_24hrs[0,qq] = subm_24hrs[qq]\n",
    "    \n",
    "    # Similar day approach model 최적화 (몇 개의 날(N)을 가져오는 게 좋은 지 평가.)    \n",
    "    prev_smape = 200\n",
    "    fsim = 0  # N개의 날 \n",
    "    for sim_len in range(2, 5):    \n",
    "        sim_smape,  fcst_sim = similar_approach(trainAR, testAR, sim_len, temp_24hrs)\n",
    "        if prev_smape>sim_smape:\n",
    "            fsim = sim_len\n",
    "            prev_smape = sim_smape\n",
    "            \n",
    "    # Similar day approach model       \n",
    "    sim_smape,  fcst_sim = similar_approach(trainAR, testAR, fsim, temp_24hrs)\n",
    "    # ---------------------------------------------------------------------------------------    \n",
    "    \n",
    "    minor_idx = 0 # Autoregression model에서 minor value가 나타나면, \n",
    "    # 모델을 Autoregression model에서 similar day appreach로 변경 진행.\n",
    "    \n",
    "    # SMAPE가 linear model이 가장 작으면, 해당 결과 사용\n",
    "    if (lin_sampe<non_smape)&(lin_sampe<Mac_smape)&(lin_sampe<sim_smape):    \n",
    "        fcst = np.zeros([1,24])  \n",
    "        for qq in range(0,24):\n",
    "            fcst[0,qq] = fcst_temp[qq]\n",
    "            \n",
    "            if fcst_temp[qq]<0:\n",
    "                minor_idx = minor_idx+1\n",
    "                \n",
    "    # SMAPE가 DNN model이 가장 작으면, 해당 결과 사용\n",
    "    if (non_smape<lin_sampe)&(non_smape<Mac_smape)&(non_smape<sim_smape):\n",
    "        temp_24hrs = np.zeros([1,24])\n",
    "        for qq in range(0,24):\n",
    "            temp_24hrs[0,qq] = subm_24hrs[qq]\n",
    "            \n",
    "        fcst = Non_NNmodel.predict(temp_24hrs)\n",
    "    \n",
    "    # SMAPE가 random forest model이 가장 작으면, 해당 결과 사용\n",
    "    if (Mac_smape<non_smape)&(Mac_smape<lin_sampe)&(Mac_smape<sim_smape):\n",
    "        fcst = mac_fcst\n",
    "    \n",
    "    # SMAPE가 Similar day approach model이 가장 작으면, 해당 결과 사용        \n",
    "    if (sim_smape<non_smape)&(sim_smape<lin_sampe)&(sim_smape<Mac_smape):\n",
    "        fcst = fcst_sim\n",
    "        \n",
    "    if (minor_idx>0):\n",
    "        fcst = fcst_sim\n",
    "    \n",
    "    # 각 SMAPE 결과 값을 정    \n",
    "    comp_smape.append([non_smape, lin_sampe,Mac_smape, sim_smape])\n",
    "\n",
    "    \n",
    "    a = pd.DataFrame() # a라는 데이터프레임에 예측값을 정리합니다.\n",
    "    \n",
    "    print('Section [4]: Hour prediction model...............')    \n",
    "    for i in range(24):\n",
    "        a['X2018_7_1_'+str(i+1)+'h']=[fcst[0][i]] # column명을 submission 형태에 맞게 지정합니다.\n",
    "        \n",
    "\n",
    "#%% Section #5: Day prediction\n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# < fcst_d >: Predicted day profile (result)\n",
    "#------------------------------------------------------------------------------ \n",
    "    fcst_d = np.zeros([1,10])       # pre-allocation of the result data\n",
    "    trainAR_Day = AR_day_set(test, key)  #데이터를 불러옵니다.\n",
    "    \n",
    "    # Similar day aprroach\n",
    "    day_idx = np.zeros([1, 10])\n",
    "    for ii in range(0, 10):\n",
    "        mod_idx = -1\n",
    "        temp_idx =  (ii+mod_idx) % 7     #예측하는 날에 맞는 요일 idx를 정리.\n",
    "        day_idx[0, ii]  = temp_idx\n",
    "        \n",
    "    for ii in range(0,10):\n",
    "        flen = np.random.randint(3)+2    # 2~5개까지 랜덤하게 과거 데이터를 불러옵니다.\n",
    "        temp_day = np.zeros([1, flen])\n",
    "        for jj in range(0, flen):  \n",
    "            temp_day[0,jj] =  trainAR_Day[jj, int(round(day_idx[0, ii]))]\n",
    "        \n",
    "        # 불러온 데이터를 평균을하여 예측함.\n",
    "        fcst_d[0,ii] = np.mean(temp_day)\n",
    "    print('Section [5]: Day prediction model...............')\n",
    "    \n",
    "    for i in range(10):\n",
    "        a['X2018_7_'+str(i+1)+'_d']=[fcst_d[0][i]] # column명을 submission 형태에 맞게 지정합니다.\n",
    "    \n",
    "    del mod_idx, temp_idx\n",
    "#%% Section #6: Month prediction\n",
    "#------------------------------------------------------------------------------\n",
    "# [Output]\n",
    "# < pred_(N)m >: N번째 달의 전력 사용량 예측 결과\n",
    "#------------------------------------------------------------------------------ \n",
    "    mon_test = np.zeros([1,300])\n",
    "    \n",
    "    # Similar day aprroach \n",
    "    day_idx = np.zeros([1, 300])\n",
    "    for ii in range(0, 300):\n",
    "        mod_idx = -1\n",
    "        temp_idx = (ii+mod_idx) % 7  # 요일 idx 생성(월~일: 0~6)\n",
    "        day_idx[0, ii]  = temp_idx\n",
    "    \n",
    "    # 휴일의 경우, 일요일과 같은 데이터로 가정함.\n",
    "    day_idx[0,31+15-1] = 6 # 광복절\n",
    "    day_idx[0,31+31+24-1] = 6 # 추석\n",
    "    day_idx[0,31+31+25-1] = 6 # 추석\n",
    "    day_idx[0,31+31+26-1] = 6 # 대체휴일\n",
    "    day_idx[0,31+31+30+3-1] = 6 # 개천절\n",
    "    day_idx[0,31+31+30+9-1] = 6 # 한글날\n",
    "    day_idx[0,31+31+30+31+30+25-1] = 6 # 성탄절\n",
    "    \n",
    "    for ii in range(0,300):\n",
    "        flen = np.random.randint(3)+1  # Similar day approach를 위한 1~4개의 데이터 추출\n",
    "        temp_day = np.zeros([1, flen])\n",
    "        for jj in range(0, flen):  \n",
    "             temp_day[0,jj] =  trainAR_Day[jj, int(round(day_idx[0, ii]))]\n",
    "            \n",
    "        mon_test[0,ii] = np.mean(temp_day)\n",
    "\n",
    "    # 결과 합         \n",
    "    pred_7m = np.sum(mon_test[0,0:31])\n",
    "    pred_8m = np.sum(mon_test[0,31:62])\n",
    "    pred_9m = np.sum(mon_test[0,62:92])\n",
    "    pred_10m = np.sum(mon_test[0,92:123])\n",
    "    pred_11m = np.sum(mon_test[0,123:153])\n",
    "    \n",
    "    a['X2018_7_m'] = [pred_7m] # 7월\n",
    "    a['X2018_8_m'] = [pred_8m] # 8월\n",
    "    a['X2018_9_m'] = [pred_9m] # 9월\n",
    "    a['X2018_10_m'] = [pred_10m] # 10월\n",
    "    a['X2018_11_m'] = [pred_11m] # 11월 \n",
    "  \n",
    "    a['meter_id'] = key \n",
    "    agg[key] = a[submission.columns.tolist()]  \n",
    "    \n",
    "    print('Section [6]: Month prediction model...............')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section [7]: Saving the result files...............\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% Section #6: Write the result data...\n",
    "# makes the result file\n",
    "# [File name]: dacon_submmision.csv\n",
    "\n",
    "output1 = pd.concat(agg, ignore_index=False)\n",
    "output2 = output1.reset_index().drop(['level_0','level_1'], axis=1)\n",
    "output2['id'] = output2['meter_id'].str.replace('NX','').astype(int)\n",
    "output2 =  output2.sort_values(by='id', ascending=True).drop(['id'], axis=1).reset_index(drop=True)\n",
    "output2.to_csv('sub_baseline.csv', index=False)\n",
    "\n",
    "print('Section [7]: Saving the result files...............')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![모델링1](img/모델링1.png)\n",
    "![모델링2](img/모델링2.png)\n",
    "![ar모델](img/ar모델.png)\n",
    "![similar모델](img/similar모델.png)\n",
    "![rn_and_dnn모델](img/rf_and_dnn모델.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
